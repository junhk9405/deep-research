// í¬ë¡¤ëŸ¬ ì¸í„°í˜ì´ìŠ¤ import
import { Crawler, SearchResult } from './crawlers';
import { generateObject } from 'ai';
import { compact } from 'lodash-es';
import pLimit from 'p-limit';
import { z } from 'zod';
import * as path from 'path';
import * as fs from 'node:fs/promises';

import { getModel, getQueryModel, getResearchModel, getReportModel, getAnswerModel, trimPrompt } from './ai/providers';
import { systemPrompt } from './prompt'; // systemPrompt import

function log(...args: any[]) {
  console.log(...args);
}

/**
 * ê²€ìƒ‰ì–´ ë©”íƒ€ë°ì´í„°ë¥¼ í†µí•© êµ¬ì¡°ì²´ë¡œ ì €ì¥
 * 1ì°¨/2ì°¨ ê²€ìƒ‰ì„ ëª¨ë‘ í•˜ë‚˜ì˜ íŒŒì¼ì— ëˆ„ì  ì €ì¥
 */
async function saveSearchQueries(
  folderPath: string, 
  queries: any[], 
  depth: number,
  maxDepth: number,
  parentDimension?: string
) {
  try {
    // í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™ (report/í”„ë¡œì íŠ¸ëª…/)
    const projectRootPath = path.dirname(folderPath);
    const queryFilePath = path.join(projectRootPath, 'queries.json');
    const projectName = path.basename(projectRootPath);
    
    // ê¸°ì¡´ íŒŒì¼ ì½ê¸° (ì—†ìœ¼ë©´ ì´ˆê¸° êµ¬ì¡° ìƒì„±)
    let searchData: any;
    try {
      const existingData = await fs.readFile(queryFilePath, 'utf-8');
      searchData = JSON.parse(existingData);
    } catch {
      // íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ êµ¬ì¡° ìƒì„±
      searchData = {
        projectInfo: {
          projectName,
          startTime: new Date().toISOString(),
          totalDepth: maxDepth
        },
        searchHistory: {
          "1ì°¨ê²€ìƒ‰": null,
          "2ì°¨ê²€ìƒ‰": {}
        }
      };
    }
    
    const currentTime = new Date().toISOString();
    const folderName = depth === maxDepth ? 'Middle' : `FollowUp_${maxDepth - depth}`;
    const isInitialSearch = depth === maxDepth;
    
    const querySection = {
      depth,
      folderName,
      timestamp: currentTime,
      parentDimension: parentDimension || null,
      totalQueries: queries.length,
      queries: queries.map((q, index) => ({
        index: index + 1,
        query: q.query,
        dimension: q.dimension || null,
        researchGoal: q.researchGoal || null,
        expectedResultFile: null
      }))
    };
    
    if (isInitialSearch) {
      // 1ì°¨ ê²€ìƒ‰: ì „ì²´ ì €ì¥
      searchData.searchHistory["1ì°¨ê²€ìƒ‰"] = querySection;
    } else {
      // 2ì°¨ ê²€ìƒ‰: í•´ë‹¹ dimensionì—ë§Œ ì €ì¥
      const dimensionKey = parentDimension
        ? parentDimension.replace(/[^a-zA-Z0-9ê°€-í£]/g, '').replace(/\s+/g, '')
        : `Unknown_${Date.now()}`;
      
      searchData.searchHistory["2ì°¨ê²€ìƒ‰"][dimensionKey] = querySection;
    }
    
    // íŒŒì¼ ì €ì¥
    await fs.writeFile(queryFilePath, JSON.stringify(searchData, null, 2), 'utf-8');
    log(`âœ… Search queries saved: ${queryFilePath} (${isInitialSearch ? '1ì°¨ê²€ìƒ‰' : '2ì°¨ê²€ìƒ‰-' + parentDimension})`);
    
  } catch (error) {
    console.warn(`âš ï¸ Failed to save search queries: ${error}`);
  }
}

export type ResearchProgress = {
  currentDepth: number;
  totalDepth: number;
  currentBreadth: number;
  totalBreadth: number;
  currentQuery?: string;
  totalQueries: number;
  completedQueries: number;
};

export type ResearchResult = {
  learnings: string[];
  visitedUrls: string[];
};

// increase this if you have higher API rate limits
const ConcurrencyLimit = Number(process.env.CRAWLER_CONCURRENCY) || 3;

// í¬ë¡¤ëŸ¬ íƒ€ì… (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
const crawlerType = process.env.CRAWLER_TYPE || 'crawl4ai';

// ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
console.log('í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì •ë³´:');
console.log('í¬ë¡¤ëŸ¬ íƒ€ì…:', crawlerType);

// í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ (lazy loading)
let crawlerInstance: Crawler | null = null;

// í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
async function getCrawlerInstance(): Promise<Crawler> {
  if (!crawlerInstance) {
    const { getCrawler } = await import('./crawlers');
    crawlerInstance = await getCrawler(crawlerType as any);
  }
  return crawlerInstance;
}

// take en user query, return a list of SERP queries, ì¼ë‹¨ numqueriesëŠ” 3ê°œì—ì„œ ë³€ê²½í•¨. 
export async function generateSerpQueries({
  query,
  numQueries = 3,
  learnings,
  useDimensions = true,
  parentDimension, // ë””ë©˜ì…˜ ì‚¬ìš© ì—¬ë¶€ (1ì°¨ ê²€ìƒ‰ì—ì„œëŠ” true, 2ì°¨ ê²€ìƒ‰ì—ì„œëŠ” false)
}: {
  query: string;
  numQueries?: number;
  // optional, if provided, the research will continue from the last learning
  learnings?: string[];
  useDimensions?: boolean;
  parentDimension?: string;
}) {
    // âœ… 1. ë¨¼ì € ê²€ìƒ‰ ëª¨ë“œ íŒë³„
  const isFirstLevel = !parentDimension || parentDimension.trim() === '';

  // âœ… 2. í”„ë¡¬í”„íŠ¸ ë¸”ë¡ ì •ì˜
  const firstLevelPrompt = `You are conducting an initial broad search to explore the topic across the 5 core strategic dimensions listed below.

    Generate 5 search queries in English â€” one for each of the following business-critical strategic dimensions:
    TOPIC ANCHOR
    Each query MUST:
      â€¢ Add only dimension-specific terms that relate directly to this topic.
      â€¢ Exclude unrelated or broader synonyms.

    Solution Overview
    â€” solution scope and functional definition, Korean-language search insights on competing offerings, evidence of unmet needs and pain-point intensity, and the high-level value proposition that closes the gap

    Market Landscape & Growth Dynamics
    â€” global & regional TAM/SAM with CAGR figures, industry or geographic break-outs, core business-value drivers and inhibitors, plus funding/M&A trends shaping adoption timing

    Customer Segmentation & Demand Analysis
    â€” segmentation taxonomy and key personas, quantified pain-point severity (frequency Ã— cost), willingness-to-pay indicators, and adoption triggers across size, industry, or region

    Technology Assessment & Business Value
    â€” core and emerging technology options with illustrative use cases, integration complexity and benchmark references (latency, cost, team size), scalability/talent readiness, and ROI or cost-benefit differentials per technology

    Risk & Regulatory Snapshot
    â€” technology and market entry barriers, data-privacy & sector-specific regulations, forthcoming compliance or AI-governance timelines, cybersecurity/ethical risks, and recommended mitigation guidelines

    Each query should view the userâ€™s topic through the lens of the corresponding strategic dimension.`; // (ìœ„ ë‚´ìš© ê·¸ëŒ€ë¡œ)
  const secondLevelPrompt = `You are conducting a focused second-level search on the dimension: **${parentDimension}**
    Generate ${numQueries} follow-up search queries in English (3â€“10 words each) that:
    - Generate search queries by selecting one sub-topic under each **"${parentDimension}"** below:
    Solution Overview
    1-1. Solution definition & scope (what the solution does)
    1-2. Korean-language search findings and competitive snapshot
    1-3. Evidence of unmet needs & high-level value proposition

    Market Landscape & Growth Dynamics
    2-1. Global and Korean TAM/SAM with CAGR figures
    2-2. Business value drivers and inhibitors shaping adoption
    2-3. Investment, funding and M&A trends across the value chain

    Customer Segmentation & Demand Analysis
    3-1. Customer Segmentation (industry, size, geography)
    3-2. Current pain points / jobs-to-be-done and severity metrics
    3-3. Quantified demand, willingness-to-pay and adoption triggers

    Technology Assessment & Business Value
    4-1. Core and emerging technologies with illustrative use cases
    4-2. Implementation complexity & benchmark references (latency, cost, team size)
    4-3. Business value per technology (ROI, cost-savings, revenue lift)

    Risk & Regulatory Snapshot
    5-1. Technology entry barriers and inherent limitations
    5-2. Market entry barriers and competitive moats
    5-3. Regulatory, compliance and policy risks
    `; // (ìœ„ ë‚´ìš© ê·¸ëŒ€ë¡œ)
  const res = await generateObject({
    model: getQueryModel(),
    system: systemPrompt(),
    prompt: `Given the user query, extract key concepts and generate simple, broad search queries that cover the topic comprehensively. 
User Query: ${query}
Parent Dimension: ${parentDimension}

Step 1 â€” Determine the search mode:

- If **parentDimension is not provided** (i.e., it is null or empty), this is an **initial broad exploration** step.  
  You should generate **one query per strategic dimension** to explore the topic from all relevant angles.
  Please Be sure to stay focused on the user's topic. 
  
- If **parentDimension is provided**, it indicates the current research is a **follow-up deep dive** focused on that specific strategic dimension.  
  You should generate **multiple follow-up queries** targeting only that dimension.

Regardless of the mode, follow these principles:
- Always include the topic keyword from the original query in every generated search query.
- Keep each query short and simple (min 4 ~ max 12 words)
- Avoid redundancy â€” do not repeat similar queries or rephrase the same idea
- Ensure each query targets a **unique, meaningful aspect** of the topic

${(isFirstLevel ? firstLevelPrompt : secondLevelPrompt)}

${learnings ? 
  `\nPrevious research context:\n${learnings.slice(0, 2).join('\n\n')}\n\nBased on this context, generate queries that explore gaps or expand on key findings.` 
  : ''
}`,
    schema: z.object({
      queries: useDimensions 
        ? z.array(
            z.object({
              dimension: z.enum([
                "Solution Overview",
                "Market Landscape & Growth Dynamics",
                "Customer Segmentation & Demand Analysis",
                "Technology Assessment & Business Value",
                "Risk & Regulatory Snapshot"
              ]).describe("The strategic dimension this query focuses on."),
              query: z.string().describe('Simple, broad search query (3-7 words, covering key aspects of the topic).'),
              researchGoal: z
                .string()
                .describe(
                  'Brief goal of this research query and what insights we expect to find.',
                ),
            })
          )
          .length(5) // 1ì°¨ ê²€ìƒ‰: 5ê°€ì§€ ë””ë©˜ì…˜ ê°•ì œ
          .describe(`List of 5 simple, broad SERP queries covering different strategic aspects.`)
        : z.array( // 2ì°¨ ê²€ìƒ‰: dimension í•„ë“œ ì™„ì „ ì œê±°
            z.object({
              query: z.string().describe('Simple follow-up search query (3-10 words).'),
              researchGoal: z.string().describe('Brief goal of this follow-up research.'),
            })
          )
          .min(1)
          .max(numQueries)
          .describe(`List of simple follow-up queries for deeper research.`),
    }),
  });
  log(`Created ${res.object.queries.length} queries`, res.object.queries);

  return res.object.queries.slice(0, numQueries);
}
// numLearnings, Fup Questions ëŠ” 2ê°œë¡œ ë³€ê²½
export async function processSerpResult({
  query,
  result,
  numLearnings = 3,
  numFollowUpQuestions = 3,
}: {
  query: string;
  result: SearchResult;
  numLearnings?: number;
  numFollowUpQuestions?: number;
}) {
  // 1. ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
  log(`[DEBUG] Processing SERP result for query: "${query}"`);
  log(`[DEBUG] Raw result data length: ${result.data?.length || 0}`);
  log(`[DEBUG] Raw result structure:`, JSON.stringify(result, null, 2).substring(0, 500) + '...');
  
  // 2. ê²€ìƒ‰ ê²°ê³¼(markdown) ì¶”ì¶œ - ìµœëŒ€ 5ê°œë¡œ ì œí•œ
  const contents = compact(result.data.map(item => item.markdown)).slice(0, 10);
  log(`[DEBUG] Extracted contents length: ${contents.length} (limited to 5)`);
  
  // ê° ì½˜í…ì¸ ì˜ ê¸¸ì´ë„ í™•ì¸
  contents.forEach((content, index) => {
    log(`[DEBUG] Content ${index + 1} length: ${content?.length || 0} chars`);
  });
  
  // ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ê²°ê³¼ ë°˜í™˜
  if (contents.length === 0) {
    log(`[ERROR] No contents found for query: ${query}`);
    log(`[ERROR] Available data fields:`, Object.keys(result.data[0] || {}));
    return {
      learnings: [],
      followUpQuestions: [],
    };
  }

  // 2. ê° ë¬¸ì„œë³„ ê°œë³„ ìš”ì•½ ìƒì„± - ë™ì‹œì„± ì œí•œìœ¼ë¡œ ìˆœì°¨ ì²˜ë¦¬
  log(`Generating individual summaries for ${contents.length} documents...`);
  const docLimit = pLimit(3); // ë¬¸ì„œ ì²˜ë¦¬ ë™ì‹œì„±ì„ 5ê°œë¡œ ì œí•œ
  const learningsPerDoc = await Promise.all(
    contents.map((content, index) =>
      docLimit(async () => {
        try {
          // ê° ë¬¸ì„œ ë‚´ìš© ê¸¸ì´ ì œí•œ - í† í° ìˆ˜ ì¶•ì†Œ
          const trimmedContent = trimPrompt(content, 50_000);
          
          // ê°œë³„ ë¬¸ì„œì— ëŒ€í•œ ìš”ì•½ ìƒì„±
          const docRes = await generateObject({
            model: getResearchModel(),
            abortSignal: AbortSignal.timeout(60_000), // íƒ€ì„ì•„ì›ƒ ì¦ê°€
            system: systemPrompt(),
            prompt: `Given the following content from a search result for the query "${query}", generate at least more than 10 key learnings from this specific document. Make the learnings precise, detailed and information-dense. Include any entities, metrics, numbers, or dates.

<content>
${trimmedContent}
</content>`,
            schema: z.object({
              documentLearnings: z.array(z.string()).describe('key learnings from this document'),
            }),
          });
          
          log(`Document ${index + 1}: Generated ${docRes.object.documentLearnings.length} learnings`);
          return docRes.object.documentLearnings;
        } catch (error) {
          log(`Error processing document ${index + 1}:`, error);
          return []; // ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë°°ì—´ ë°˜í™˜í•˜ì—¬ ê³„ì† ì§„í–‰
        }
      })
    ),
  );

  // 3. ëª¨ë“  ê°œë³„ ìš”ì•½ í†µí•© (í‰ë©´í™”)
  const allDocumentLearnings = learningsPerDoc.flat();
  log(`[DEBUG] Total individual learnings generated: ${allDocumentLearnings.length}`);

  // 4. ë¹ˆ ë°ì´í„° ì²˜ë¦¬ - ê°œë³„ í•™ìŠµ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ì•ˆì „ì¥ì¹˜
  if (allDocumentLearnings.length === 0) {
    log(`[ERROR] No individual learnings generated for query: ${query}`);
    return {
      learnings: [`## ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\n\nê²€ìƒ‰ì–´ "${query}"ì— ëŒ€í•œ ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ê²€ìƒ‰ ë°©ì‹ì„ ì‹œë„í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.`],
      followUpQuestions: [`${query}ì™€ ê´€ë ¨ëœ ë‹¤ë¥¸ ê²€ìƒ‰ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?`],
    };
  }

  // 5. ì „ì²´ í†µí•© ìš”ì•½ ìƒì„± (ì¤„ê¸€ í˜•ì‹)
  const res = await generateObject({
    model: getResearchModel(),
    abortSignal: AbortSignal.timeout(90_000), // ìƒì„¸í•œ ì‘ì„±ì„ ìœ„í•´ íƒ€ì„ì•„ì›ƒ ì¦ê°€
    system: systemPrompt(),
    prompt: trimPrompt(
      `Using only the sentences inside <individual_learnings>, write a coherent narrative for the search query: â€œ${query}.â€
      You are a precise information extractor. Your primary goal is accuracy over length.

      ## Core Principles
      1. **Only use information explicitly present in sources**
      2. **Never generate content beyond what sources provide**
      3. **Quality over quantity - shorter accurate content is better than longer fabricated content**
      4. **Output in English unless otherwise specified**

      ## Extraction Rules

      1. **Information Gathering**
        - Extract meaningful chunks of information (2-4 sentences) that form complete thoughts
        - Combine directly related facts from the same source into coherent paragraphs
        - Keep statistical data, quotes, and specific claims intact
        - Preserve important context that appears immediately around key facts

      2. **Expansion Guidelines** (ONLY when source material allows)
        - If sources contain explanations, include them
        - If sources show cause-effect relationships, preserve them
        - If sources provide examples or comparisons, retain them
        - DO NOT create explanations, examples, or relationships not in sources

      3. **Structure**
        - Use ## topical headings based on actual content available
        - Under each heading:
          * Group related information from sources
          * Maintain logical flow by ordering facts sensibly
          * If only 1-2 facts exist for a topic, that's acceptable
        - Skip sections entirely if insufficient information exists

      4. **Length Management**
        - Target length: As long as source material supports (typically 600-1500 words)
        - Each section: Include what sources provide, no minimum requirement
        - If a topic has limited information, keep it brief
        - Never pad content to meet length targets

      5. **Anti-Hallucination Safeguards**
        - Before writing each sentence, verify it exists in sources
        - Use these markers when needed:
          * "[ì •ë³´ ì—†ìŒ]" - when expected information is missing
          * "[ì¶”ê°€ ì •ë³´ ë¶€ì¡±]" - when a topic seems incomplete
          * "[ì›ë¬¸ ì°¸ì¡° í•„ìš”]" - when sources hint at more detail not provided
        - If unsure whether information is from sources or inference, exclude it

      6. **Citation Rules** 
        1. Format â€“ Append citations as (Source: â€¦) or (Sources: â€¦, â€¦) after each fact-bearing sentence or paragraph.
        2. Acceptable source strings MUST satisfy at least one of the following:
            a) Widely recognized organisation, company, or government body
              e.g. Gartner, OECD, Microsoft
            b) Established journal, conference, or media outlet with optional year
              e.g. Nature 2024, IEEE ICCV 2023, MIT Technology Review
            c) Clear top-level domain of a reputable entity
              e.g. who.int, nasa.gov, ft.com, arxiv.org
        3. Special cases
            â€¢ arXiv cite as arXiv 2409.14858 2024 or arXiv.org â€“ Paper Title
            â€¢ Datasets or repositories include provider and dataset or repo name
              e.g. Kaggle â€“ Titanic Survival
        4. Disallowed identifiers omit the statement if only these are available:
            â€¢ Pure numbers or version strings 2409.14858v1, 2020, 1, 3, 5
            â€¢ Random codes or hashes SRTE57145DR, abc123
            â€¢ Generic words content, article, data, report
        5. Multiple sources separate with commas
            e.g. (Sources: Gartner 2024, arXiv 2409.14858 2024)
        6. Authority first favour facts backed by authoritative, reputable sources; ignore information that cannot be cited with a compliant identifier.

      7. **Information Density Check**
        - Acceptable to have short sections if sources are limited
        - Acceptable to have fewer sections if content is sparse
        - NOT acceptable to invent content to fill space
        - Focus on extracting all available valuable information rather than creating narrative

      ## Final Verification
      Before outputting, check each statement:
      - Can I point to the exact source sentence(s)?
      - Am I combining facts or creating new connections not in sources?
      - Would removing this statement lose source information?

      Remember: Your users prefer complete but concise information over lengthy but partially fabricated content..
      
      \\n\n<individual_learnings>\n${allDocumentLearnings.map(learning => `- ${learning}`).join('\n')}\n</individual_learnings>`,
    ),
    schema: z.object({
      detailedSummary: z.string().describe('A detailed, structured summary in Korean with appropriate section headers (## format). Each section should contain comprehensive narrative content while preserving original information details.'),
      followUpQuestions: z
        .array(z.string())
        .describe(
          `List of follow-up questions to research the topic further, max of ${numFollowUpQuestions}`,
        ),
    }),
  });

  // ìƒì„±ëœ ìƒì„¸ ìš”ì•½ë¬¸ì„ ë°°ì—´ì— ë‹´ì•„ ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ í•¨
  const learnings = [res.object.detailedSummary];
  log(`[DEBUG] Created a detailed summary and ${res.object.followUpQuestions.length} questions`);
  
  return {
    learnings,
    followUpQuestions: res.object.followUpQuestions,
  };
}

export async function writeFinalReport({
  prompt,
  learnings,
  visitedUrls,
}: {
  prompt: string;
  learnings: string[];
  visitedUrls: string[];
}) {
  const learningsString = learnings
    .map(learning => `<learning>\n${learning}\n</learning>`)
    .join('\n');

  const res = await generateObject({
    model: getReportModel(),
    system: systemPrompt(),
    prompt: trimPrompt(
      `ì´ì „ ê²€ìƒ‰ì—ì„œ ì–»ì€ ëª¨ë“  ì¡°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì£¼ì œì— ëŒ€í•œ ì‹¬ì¸µ ê¸°ìˆ  ì „ëµ ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. 
       ê²€ìƒ‰ ë‚´ìš©ì—ëŠ” ë¶ˆí•„ìš”í•œ ë‚´ìš© í˜¹ì€ ì£¼ì œì™€ ë¬´ê´€í•œ ë‚´ìš©ë„ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, í•„ìš”í•œ ì •ë³´ë¥¼ ì˜ ì·¨ì‚¬ì„ íƒí•˜ëŠ” ê²ƒë„ ë‹¹ì‹ ì˜ ëŠ¥ë ¥ì…ë‹ˆë‹¤.

      You are an AI Research Agent assigned to prepare an in-depth, structured report to support high-level business decision-making for a specific IT solution.

      The report must be comprehensive, evidence-based, and reflect all relevant research findings. It should include a minimum of 15 pages of detailed content, organized under the following sections:

      ---

      ### Report Structure & Section Goals

      1. **Executive Summary & Strategic Rationale**  
        - One-sentence mission & â€œwhy nowâ€ argument  
        - Three key take-aways / decisions

      2. **Market & Competitive Insights**  
        - Market size & CAGR   
        - Demand drivers / blockers & customer pain points  
        - 3 leading competitors & white-space  

      3. **Technology Assessment & Fit-Gap**  
        - Two or three pivotal tech trends 
        - Capabilities-vs-requirements  (build / buy / partner)  
        - Integration feasibility & scalability constraints  

      4. **Business Value & ROI Outlook**  
        - Cost-saving & revenue-uplift levers 
        - High-level financial model: base vs. stretch scenario  
        - Payback period & two to three headline KPIs  

      5. **High-Level Roadmap (12â€“36 months)**  
        - Phase 0: PoC goal & success metric  
        - Phase 1: MVP launch (quarter, budget, team size)  
        - Phase 2: Scale & optimize (optional)  

      6. **Risks & Governance**  
        - Top five risks (tech / market / compliance) with mitigations  
        - Ownership (RACI) snapshot & monthly KPI review cadence  

      ---
      Strictly follow these writing guidelines for every section:
      - Use Markdown headings for each main section
      - Each Sub-sections should include at least 2â€“3 full paragraphs.
      - Each section should contain at least 4â€“5 detailed paragraphs.
      - Important!: Every paragraph must adopt a **Claim** â†’ **Multiple Evidences** flow
      - In the Evidences part, **provide as many distinct data points, studies, expert quotes, or case examples as are credibly available (3ê°œ ì´ìƒì´ ë°”ëŒì§)**, weaving them into a persuasive narrative rather than a mere list.
      - Each paragraph must have written in well-formed, narrative sentences.
      - Provide concrete explanations and deep analysis, not surface-level summaries.
      - Include quantitative data, statistics, industry examples, and use structured elements like tables and bullet lists where appropriate
            - In the Roadmap section, develop clear short-/mid-/long-term execution plans (not just listing facts)
            - The combined word count of tables/figures must NOT exceed 30% of the section.

      All information must be synthesized into original insightâ€”not copied or paraphrased. Avoid bullet-style or brief summaries as the main format. Every section must contain rich explanation and thoughtful discussion.
      **Please write your response in Korean.**

      Here is the result of All learnings. <learnings>\n${learningsString}\n</learnings>
      `,
    ),
    schema: z.object({
      reportMarkdown: z.string().describe('Comprehensive, detailed strategic report on the topic in Markdown format with extensive analysis and insights'),
    }),
  });

  // Append the visited URLs section to the report
  const urlsSection = `\n\n## Sources\n\n${visitedUrls.map(url => `- ${url}`).join('\n')}`;
  return res.object.reportMarkdown + urlsSection;
}

export async function writeFinalAnswer({
  prompt,
  learnings,
}: {
  prompt: string;
  learnings: string[];
}) {
  const learningsString = learnings
    .map(learning => `<learning>\n${learning}\n</learning>`)
    .join('\n');

  const res = await generateObject({
    model: getAnswerModel(),
    system: systemPrompt(),
    prompt: trimPrompt(
      `Given the following prompt from the user, write a final answer on the topic using the learnings from research. Follow the format specified in the prompt. Do not yap or babble or include any other text than the answer besides the format specified in the prompt. Keep the answer as concise as possible - usually it should be just a few words or maximum a sentence. Try to follow the format specified in the prompt (for example, if the prompt is using Latex, the answer should be in Latex. If the prompt gives multiple answer choices, the answer should be one of the choices).\n\n<prompt>${prompt}</prompt>\n\nHere are all the learnings from research on the topic that you can use to help answer the prompt:\n\n<learnings>\n${learningsString}\n</learnings>`,
    ),
    schema: z.object({
      exactAnswer: z
        .string()
        .describe('The final answer, make it short and concise, just the answer, no other text'),
    }),
  });
  return res.object.exactAnswer;
}

export async function deepResearch({
  solutionContext, // ì´ ë¶€ë¶„ì´ solutionContextë¡œ ì‚¬ìš©ë¨
  query, // ì´ ë¶€ë¶„ì´ initialQueryë¡œ ì‚¬ìš©ë¨
  breadth,
  depth,
  learnings = [], // ì´ì „ depthì—ì„œ ëˆ„ì ëœ learnings
  visitedUrls = [], // ì´ì „ depthì—ì„œ ëˆ„ì ëœ visitedUrls
  onProgress,
  initialQuery, // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ë³´ì¡´ìš©
  originalDepth, // ìµœì´ˆ depth ê°’ ë³´ì¡´ìš©
  parentDimension, // 1ì°¨ ê²€ìƒ‰ì˜ ë””ë©˜ì…˜ ì •ë³´
}: {
  solutionContext?: string;
  query: string;
  breadth: number;
  depth: number;
  learnings?: string[];
  visitedUrls?: string[];
  onProgress?: (progress: ResearchProgress) => void;
  initialQuery?: string; // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ë³´ì¡´ìš©
  originalDepth?: number; // ìµœì´ˆ depth ê°’ ë³´ì¡´ìš©
  parentDimension?: string; // 1ì°¨ ê²€ìƒ‰ì˜ ë””ë©˜ì…˜ ì •ë³´
}): Promise<ResearchResult> {
  // originalDepthê°€ ì—†ìœ¼ë©´ í˜„ì¬ depthë¥¼ ìµœì´ˆ depthë¡œ ì„¤ì •
  const maxDepth = originalDepth || depth;
  
  const progress: ResearchProgress = {
    currentDepth: depth,
    totalDepth: maxDepth, // ìµœì´ˆ depth ê°’ ì‚¬ìš©
    currentBreadth: breadth,
    totalBreadth: breadth,
    totalQueries: 0,
    completedQueries: 0,
  };

  const reportProgress = (update: Partial<ResearchProgress>) => {
    Object.assign(progress, update);
    onProgress?.(progress);
  };

  // 1ì°¨ ê²€ìƒ‰ì€ useDimensions: true, 2ì°¨ ê²€ìƒ‰(depth < maxDepth)ì€ useDimensions: false
  const isInitialSearch = depth === maxDepth;
  
  const serpQueries = await generateSerpQueries({
    query,
    learnings,
    numQueries: breadth,
    useDimensions: isInitialSearch, // 2ì°¨ ê²€ìƒ‰ë¶€í„°ëŠ” ë””ë©˜ì…˜ ì‚¬ìš© ì•ˆ í•¨
    parentDimension, // parentDimension ì „ë‹¬ ì¶”ê°€
  });

  reportProgress({
    totalQueries: serpQueries.length,
    currentQuery: serpQueries[0]?.query,
  });

  const limit = pLimit(ConcurrencyLimit);

  // í•˜ìœ„ ë³´ê³ ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± ë¡œì§ ì¶”ê°€
  // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë”ëª… ìƒì„± (ì¬ê·€ í˜¸ì¶œ ì‹œì—ë„ ë™ì¼í•œ í´ë” ì‚¬ìš©)
  const queryForFolder = initialQuery || query;
  let safeInitialQuery = queryForFolder.replace(/[\/\?%\*:\|"<>\.]/g, '').replace(/\s+/g, '_');
  // Windowsì˜ MAX_PATH ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë””ë ‰í† ë¦¬ ì´ë¦„ ë¶€ë¶„ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
  // MAX_PATHëŠ” ì•½ 260ìì´ë©°, ê¸°ë³¸ ê²½ë¡œ, 'report', '<ê²€ìƒ‰ì–´>', 'Middle', ì¼ë°˜ì ì¸ íŒŒì¼ ì´ë¦„ì„ ì œì™¸í•˜ë©´
  // ì´ ë¶€ë¶„ì— ì•½ 150ì ì •ë„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  const MAX_DIR_PART_LENGTH = 100;
  if (safeInitialQuery.length > MAX_DIR_PART_LENGTH) {
    safeInitialQuery = safeInitialQuery.substring(0, MAX_DIR_PART_LENGTH);
    // ì˜ë¼ë‚¸ í›„ ë§ˆì§€ë§‰ì— ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” '_' ë¬¸ìë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
    safeInitialQuery = safeInitialQuery.replace(/_+$/, '');
  }
  
  const getFolderName = (currentDepth: number, maxDepth: number) => {
    if (currentDepth === maxDepth) {
      return 'Middle'; // 1ì°¨ ê²€ìƒ‰ (ìµœì´ˆ depth)
    } else {
      const followUpLevel = maxDepth - currentDepth;
      return `FollowUp_${followUpLevel}`; // 2ì°¨: FollowUp_1, 3ì°¨: FollowUp_2, ...
    }
  };
  
  const folderName = getFolderName(depth, maxDepth);
  const subReportDir = path.join('report', safeInitialQuery, folderName);
  await fs.mkdir(subReportDir, { recursive: true });

  // ğŸ” ê²€ìƒ‰ì–´ ë©”íƒ€ë°ì´í„° ì €ì¥ (ê¸°ì¡´ ì½”ë“œì— ì˜í–¥ ì—†ìŒ)
  await saveSearchQueries(subReportDir, serpQueries, depth, maxDepth, parentDimension);

  const results = await Promise.all(
    serpQueries.map(serpQuery =>
      limit(async () => {
        try {
          const crawler = await getCrawlerInstance();
          const crawlerType = process.env.CRAWLER_TYPE || 'perplexity';

          let result: SearchResult;
          if (crawlerType === 'perplexity') {
            // perplexity searchê°€ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬
            result = await crawler.search(serpQuery.query);
          } else if (crawlerType === 'firecrawl') {
            // firecrawl searchê°€ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬ (perplexityì™€ ë™ì¼)
            result = await crawler.search(serpQuery.query, {
              timeout: 30000,
              limit: 10,
              scrapeOptions: { 
                formats: ['markdown'],
                // maxTokens ì œê±° - Firecrawl v1 APIì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
              },
            });
          } else {
            // ê¸°ì¡´ í¬ë¡¤ëŸ¬ë“¤ (google, crawl4ai)ì€ search í›„ crawl í•„ìš”
            result = await crawler.search(serpQuery.query, {
              timeout: 30000, // íƒ€ì„ì•„ì›ƒ ì‹œê°„ ì¦ê°€
              limit: 10, // ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ë¥¼ 10ê°œë¡œ ì œí•œ
              scrapeOptions: { 
                formats: ['markdown'], 
                maxTokens: 100000, // í˜ì´ì§€ë‹¹ ìµœëŒ€ í† í° ìˆ˜ ì¦ê°€
              },
            });
          }

          // Collect URLs from this search
          const newUrls = compact(result.data.map(item => item.url));
          // breadth ê³„ì‚° ì›ë˜ëŒ€ë¡œ ìœ ì§€ (2ì°¨ ê²€ìƒ‰ë„ breadth ê°’ ì‚¬ìš©)
          const newBreadth = Math.ceil(breadth / 2);
          const newDepth = depth - 1;

          const newLearnings = await processSerpResult({
            // ê¸°ì¡´ processSerpResult í˜¸ì¶œ ë¶€ë¶„ ìœ ì§€
            query: serpQuery.query,
            result,
            numLearnings: 10, // ê¸°ë³¸ê°’ 5ì—ì„œ 10ìœ¼ë¡œ ì¦ê°€ (ë‚´ìš© ë³´ì¡´ì„ ìœ„í•´)
            numFollowUpQuestions: newBreadth,
          });
          const allLearnings = [...learnings, ...newLearnings.learnings];
          const allUrls = [...visitedUrls, ...newUrls];

          // í•˜ìœ„ ì£¼ì œ ê²°ê³¼ íŒŒì¼ ì €ì¥ ë¡œì§ ì¶”ê°€
          if (newLearnings.learnings.length > 0) {
            // 1ì°¨ ê²€ìƒ‰ì¸ì§€ í™•ì¸ (originalDepthì™€ í˜„ì¬ depth ë¹„êµ)
            const isInitialSearch = depth === (originalDepth || depth);
            
            // ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            if ('dimension' in serpQuery && serpQuery.dimension === 'Core Technology & Architecture') {
              console.log('\n=== CoreTechnologyArchitecture ë””ë²„ê¹… ===');
              console.log('followUpQuestions:', newLearnings.followUpQuestions);
              console.log('learnings length:', newLearnings.learnings.length);
              console.log('newDepth:', newDepth);
              console.log('newBreadth:', newBreadth);
            }
            
            // íŒŒì¼ëª… ìƒì„± ë¡œì§ ê°œì„ : ë””ë©˜ì…˜ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…
            let fileNamePrefix: string;
            
            if (isInitialSearch && 'dimension' in serpQuery && typeof serpQuery.dimension === 'string') {
              // 1ì°¨ ê²€ìƒ‰: ë””ë©˜ì…˜ ê¸°ë°˜ íŒŒì¼ëª…
              fileNamePrefix = serpQuery.dimension.replace(/[^a-zA-Z0-9_\-ê°€-í£]/g, '').replace(/\s+/g, '_');
            } else if (parentDimension) {
              // 2ì°¨ ê²€ìƒ‰: ë¶€ëª¨ ë””ë©˜ì…˜ + ì¸ë±ìŠ¤ ê¸°ë°˜ íŒŒì¼ëª…
              const dimensionSlug = parentDimension.replace(/[^a-zA-Z0-9_\-ê°€-í£]/g, '').replace(/\s+/g, '_');
              // ê°„ë‹¨í•œ ì¸ë±ìŠ¤ ìƒì„± (í˜„ì¬ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ ì„± ë³´ì¥)
              const timeIndex = Date.now().toString().slice(-4);
              const queryIndex = progress.completedQueries + 1;
              fileNamePrefix = `${dimensionSlug}_${queryIndex}_${timeIndex}`;
            } else {
              // ê¸°ë³¸ fallback
              fileNamePrefix = `Query_${progress.completedQueries + 1}`;
            }
            
            const subReportFileName = `${fileNamePrefix}.md`;
            const subReportFilePath = path.join(subReportDir, subReportFileName);
            try {
              await fs.writeFile(subReportFilePath, newLearnings.learnings.join('\n\n---\n\n'), 'utf-8');
              log(`Sub-report saved: ${subReportFilePath}`);
            } catch (writeError) {
              console.error(`Error writing sub-report ${subReportFilePath}:`, writeError);
            }
          }

          if (newDepth > 0) {
            log(`Researching deeper, breadth: ${newBreadth}, depth: ${newDepth}`);

            reportProgress({
              currentDepth: newDepth,
              currentBreadth: newBreadth,
              completedQueries: progress.completedQueries + 1,
              currentQuery: serpQuery.query,
            });

            const nextQuery = `
            Previous research goal: ${serpQuery.researchGoal}
            Follow-up research directions: ${newLearnings.followUpQuestions.map(q => `\n${q}`).join('')}
          `.trim();

            return await deepResearch({
              query: nextQuery,
              breadth: Math.ceil(breadth / 2),
              depth: depth - 1,
              learnings: allLearnings,
              visitedUrls: allUrls,
              onProgress,
              solutionContext, // ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ solutionContext ì „ë‹¬
              initialQuery: initialQuery || query, // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ì „ë‹¬
              originalDepth: maxDepth, // ìµœì´ˆ depth ê°’ ì „ë‹¬
              parentDimension: parentDimension || 
                ('dimension' in serpQuery && typeof serpQuery.dimension === 'string' 
                  ? serpQuery.dimension 
                  : undefined), // ìƒìœ„ ë””ë©˜ì…˜ì´ ìˆìœ¼ë©´ ìœ ì§€, ì—†ìœ¼ë©´ serpQuery.dimension ì‚¬ìš©
            });
          } else {
            reportProgress({
              currentDepth: 0,
              completedQueries: progress.completedQueries + 1,
              currentQuery: serpQuery.query,
            });
            return {
              learnings: allLearnings,
              visitedUrls: allUrls,
            };
          }
        } catch (e: any) {
          console.error(`[CASCADE DEBUG] Full error for query "${serpQuery.query}":`, JSON.stringify(e, null, 2));
          if (e.message && e.message.includes('Timeout')) {
            log(`Timeout error running query: ${serpQuery.query}: `, e);
          } else {
            log(`Error running query: ${serpQuery.query}: `, e);
          }
          return {
            learnings: [],
            visitedUrls: [],
          };
        }
      }),
    ),
  );

  return {
    learnings: [...new Set(results.flatMap(r => r.learnings))],
    visitedUrls: [...new Set(results.flatMap(r => r.visitedUrls))],
  };
}
