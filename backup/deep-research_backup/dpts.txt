// í¬ë¡¤ëŸ¬ ì¸í„°í˜ì´ìŠ¤ import
import { Crawler, SearchResult } from './crawlers';
import { generateObject } from 'ai';
import { compact } from 'lodash-es';
import pLimit from 'p-limit';
import { z } from 'zod';
import * as path from 'path';
import * as fs from 'node:fs/promises';

import { getModel, getQueryModel, getResearchModel, getReportModel, getAnswerModel, trimPrompt } from './ai/providers';
import { systemPrompt } from './prompt';

function log(...args: any[]) {
  console.log(...args);
}

/**
 * ê²€ìƒ‰ì–´ ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
 * ê¸°ì¡´ ì½”ë“œì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë…ë¦½ì ì¸ í•¨ìˆ˜
 */
async function saveSearchQueries(
  folderPath: string, 
  queries: any[], 
  depth: number,
  maxDepth: number,
  parentDimension?: string
) {
  try {
    const queryData = {
      depth,
      maxDepth,
      folderName: depth === maxDepth ? 'Middle' : `FollowUp_${maxDepth - depth}`,
      timestamp: new Date().toISOString(),
      parentDimension: parentDimension || null,
      totalQueries: queries.length,
      queries: queries.map((q, index) => ({
        index: index + 1,
        query: q.query,
        dimension: q.dimension || null,
        researchGoal: q.researchGoal || null,
        expectedResultFile: null // ì‹¤ì œ íŒŒì¼ëª…ì€ ë‚˜ì¤‘ì— ê²°ì •ë¨
      }))
    };
    
    const queryFilePath = path.join(folderPath, 'queries.json');
    await fs.writeFile(queryFilePath, JSON.stringify(queryData, null, 2), 'utf-8');
    log(`âœ… Search queries saved: ${queryFilePath}`);
  } catch (error) {
    // ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë©”ì¸ ë¡œì§ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ë¡œê·¸ë§Œ ì¶œë ¥
    console.warn(`âš ï¸ Failed to save search queries: ${error}`);
  }
}

export type ResearchProgress = {
  currentDepth: number;
  totalDepth: number;
  currentBreadth: number;
  totalBreadth: number;
  currentQuery?: string;
  totalQueries: number;
  completedQueries: number;
};

export type ResearchResult = {
  learnings: string[];
  visitedUrls: string[];
};

// í¬ë¡¤ëŸ¬ë³„ ë™ì‹œì„± ì œí•œ ì„¤ì •
const CRAWLER_CONCURRENCY_LIMITS = {
  firecrawl: 1,    // Firecrawlì€ Rate Limitì´ ì—„ê²©í•˜ë¯€ë¡œ 1ê°œì”©ë§Œ
  perplexity: 4,   // ê¸°ë³¸ê°’
  crawl4ai: 4,     // ê¸°ë³¸ê°’
  google: 4,       // ê¸°ë³¸ê°’
} as const;

// í¬ë¡¤ëŸ¬ íƒ€ì… (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
const crawlerType = process.env.CRAWLER_TYPE || 'crawl4ai';

// í˜„ì¬ í¬ë¡¤ëŸ¬ì˜ ë™ì‹œì„± ì œí•œ ê°€ì ¸ì˜¤ê¸°
const ConcurrencyLimit = Number(process.env.CRAWLER_CONCURRENCY) || 
  CRAWLER_CONCURRENCY_LIMITS[crawlerType as keyof typeof CRAWLER_CONCURRENCY_LIMITS] || 4;

// ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
console.log('í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì •ë³´:');
console.log('í¬ë¡¤ëŸ¬ íƒ€ì…:', crawlerType);
console.log('ë™ì‹œì„± ì œí•œ:', ConcurrencyLimit);

// í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ (lazy loading)
let crawlerInstance: Crawler | null = null;

// í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
async function getCrawlerInstance(): Promise<Crawler> {
  if (!crawlerInstance) {
    const { getCrawler } = await import('./crawlers');
    crawlerInstance = await getCrawler(crawlerType as any);
  }
  return crawlerInstance;
}

// take en user query, return a list of SERP queries, ì¼ë‹¨ numqueriesëŠ” 3ê°œì—ì„œ ë³€ê²½í•¨. 
export async function generateSerpQueries({
  query,
  numQueries = 3,
  learnings,
  useDimensions = true,
  parentDimension, // ë””ë©˜ì…˜ ì‚¬ìš© ì—¬ë¶€ (1ì°¨ ê²€ìƒ‰ì—ì„œëŠ” true, 2ì°¨ ê²€ìƒ‰ì—ì„œëŠ” false)
}: {
  query: string;
  numQueries?: number;
  // optional, if provided, the research will continue from the last learning
  learnings?: string[];
  useDimensions?: boolean;
  parentDimension?: string;
}) {
    // âœ… 1. ë¨¼ì € ê²€ìƒ‰ ëª¨ë“œ íŒë³„
  const isFirstLevel = !parentDimension || parentDimension.trim() === '';

  // âœ… 2. í”„ë¡¬í”„íŠ¸ ë¸”ë¡ ì •ì˜
  const firstLevelPrompt = `You are conducting an initial broad search to explore the topic across the six core strategic dimensions listed below.

    Generate 6 search queries in English â€” one for each of the following business-critical strategic dimensions:
    
    1. **Market Opportunity & Demand Drivers**  
      â€” market size, growth trends, customer segmentation, unmet needs, and entry barriers

    2. **Competitive Landscape & Strategic Differentiators**  
      â€” key players, technology gaps, business model differences, and positioning strategies

    3. **Business Impact & ROI Potential**  
      â€” value realization, cost-benefit comparison, and investment justification

    4. **Technology Fit & Architectural Feasibility**  
      â€” integration readiness, platform compatibility, and long-term scalability

    5. **Go-to-Market & Adoption Strategy**  
      â€” organizational readiness, rollout planning, and change management considerations

    6. **Risk Exposure & Mitigation Plan**  
      â€” known and emerging risks with mitigation references (technical, operational, regulatory)

    Each query should view the userâ€™s topic through the lens of the corresponding strategic dimension.`; // (ìœ„ ë‚´ìš© ê·¸ëŒ€ë¡œ)
  const secondLevelPrompt = `You are conducting a focused second-level search on the dimension: **${parentDimension}**
    Generate ${numQueries} follow-up search queries in English (3â€“6 words each) that:
    - Illuminate broader, decision-critical themes inside **"${parentDimension}"**
      (e.g., emerging trends, success factors, quantified benchmarks)
    - Explore diverse and non-overlapping subtopics or angles within that dimension
      (avoid repeating angles already explored)
    - Maintain an executive-level perspective: focus on insights that influence market entry,  
      competitive edge, or investment decisionsâ€”avoid overly niche or academic micro-topics
    - Build directly on the user's original query, sharpening relevance while expanding depth
    `; // (ìœ„ ë‚´ìš© ê·¸ëŒ€ë¡œ)
  const res = await generateObject({
    model: getQueryModel(),
    system: systemPrompt(),
    prompt: `Given the user query, extract key concepts and generate simple, broad search queries that cover the topic comprehensively. 
User Query: ${query}
Parent Dimension: ${parentDimension}

Step 1 â€” Determine the search mode:

- If **parentDimension is not provided** (i.e., it is null or empty), this is an **initial broad exploration** step.  
  You should generate **one query per strategic dimension** to explore the topic from all relevant angles.
  
- If **parentDimension is provided**, it indicates the current research is a **follow-up deep dive** focused on that specific strategic dimension.  
  You should generate **multiple follow-up queries** targeting only that dimension.

Regardless of the mode, follow these principles:
- Always include the topic keyword from the original query in every generated search query.
- Keep each query short and simple (min 4 ~ max 10 words)
- Avoid redundancy â€” do not repeat similar queries or rephrase the same idea
- Ensure each query targets a **unique, meaningful aspect** of the topic

${(isFirstLevel ? firstLevelPrompt : secondLevelPrompt)}

${learnings ? 
  `\nPrevious research context:\n${learnings.slice(0, 2).join('\n\n')}\n\nBased on this context, generate queries that explore gaps or expand on key findings.` 
  : ''
}`,
    schema: z.object({
      queries: useDimensions 
        ? z.array(
            z.object({
              dimension: z.enum([
                "Market Analysis & Demand Drivers",
                "Competitive Landscape & Strategic Differentiators",
                "Business Impact & ROI Potential",
                "Technology Fit & Architectural Feasibility",
                "Go-to-Market Strategy",
                "Risk Exposure & Mitigation Plan"
              ]).describe("The strategic dimension this query focuses on."),
              query: z.string().describe('Simple, broad search query (3-7 words, covering key aspects of the topic).'),
              researchGoal: z
                .string()
                .describe(
                  'Brief goal of this research query and what insights we expect to find.',
                ),
            })
          )
          .length(6) // 1ì°¨ ê²€ìƒ‰: 6ê°€ì§€ ë””ë©˜ì…˜ ê°•ì œ
          .describe(`List of 6 simple, broad SERP queries covering different strategic aspects.`)
        : z.array( // 2ì°¨ ê²€ìƒ‰: dimension í•„ë“œ ì™„ì „ ì œê±°
            z.object({
              query: z.string().describe('Simple follow-up search query (3-7 words).'),
              researchGoal: z.string().describe('Brief goal of this follow-up research.'),
            })
          )
          .min(1)
          .max(numQueries)
          .describe(`List of simple follow-up queries for deeper research.`),
    }),
  });
  log(`Created ${res.object.queries.length} queries`, res.object.queries);

  return res.object.queries.slice(0, numQueries);
}
// numLearnings, Fup Questions ëŠ” 2ê°œë¡œ ë³€ê²½
export async function processSerpResult({
  query,
  result,
  numLearnings = 3,
  numFollowUpQuestions = 3,
}: {
  query: string;
  result: SearchResult;
  numLearnings?: number;
  numFollowUpQuestions?: number;
}) {
  // 1. ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
  log(`[DEBUG] Processing SERP result for query: "${query}"`);
  log(`[DEBUG] Raw result data length: ${result.data?.length || 0}`);
  log(`[DEBUG] Raw result structure:`, JSON.stringify(result, null, 2).substring(0, 500) + '...');
  
  // 2. ê²€ìƒ‰ ê²°ê³¼(markdown) ì¶”ì¶œ - ìµœëŒ€ 5ê°œë¡œ ì œí•œ
  const contents = compact(result.data.map(item => item.markdown)).slice(0, 10);
  log(`[DEBUG] Extracted contents length: ${contents.length} (limited to 5)`);
  
  // ê° ì½˜í…ì¸ ì˜ ê¸¸ì´ë„ í™•ì¸
  contents.forEach((content, index) => {
    log(`[DEBUG] Content ${index + 1} length: ${content?.length || 0} chars`);
  });
  
  // ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ê²°ê³¼ ë°˜í™˜
  if (contents.length === 0) {
    log(`[ERROR] No contents found for query: ${query}`);
    log(`[ERROR] Available data fields:`, Object.keys(result.data[0] || {}));
    return {
      learnings: [],
      followUpQuestions: [],
    };
  }

  // 2. ê° ë¬¸ì„œë³„ ê°œë³„ ìš”ì•½ ìƒì„± - ë™ì‹œì„± ì œí•œìœ¼ë¡œ ìˆœì°¨ ì²˜ë¦¬
  log(`Generating individual summaries for ${contents.length} documents...`);
  const docLimit = pLimit(5); // ë¬¸ì„œ ì²˜ë¦¬ ë™ì‹œì„±ì„ 5ê°œë¡œ ì œí•œ
  const learningsPerDoc = await Promise.all(
    contents.map((content, index) =>
      docLimit(async () => {
        try {
          // ê° ë¬¸ì„œ ë‚´ìš© ê¸¸ì´ ì œí•œ - í† í° ìˆ˜ ì¶•ì†Œ
          const trimmedContent = trimPrompt(content, 50_000);
          
          // ê°œë³„ ë¬¸ì„œì— ëŒ€í•œ ìš”ì•½ ìƒì„±
          const docRes = await generateObject({
            model: getResearchModel(),
            abortSignal: AbortSignal.timeout(60_000), // íƒ€ì„ì•„ì›ƒ ì¦ê°€
            system: systemPrompt(),
            prompt: `Given the following content from a search result for the query "${query}", generate at least more than 10 key learnings from this specific document. Make the learnings precise, detailed and information-dense. Include any entities, metrics, numbers, or dates.

<content>
${trimmedContent}
</content>`,
            schema: z.object({
              documentLearnings: z.array(z.string()).describe('key learnings from this document'),
            }),
          });
          
          log(`Document ${index + 1}: Generated ${docRes.object.documentLearnings.length} learnings`);
          return docRes.object.documentLearnings;
        } catch (error) {
          log(`Error processing document ${index + 1}:`, error);
          return []; // ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë°°ì—´ ë°˜í™˜í•˜ì—¬ ê³„ì† ì§„í–‰
        }
      })
    ),
  );

  // 3. ëª¨ë“  ê°œë³„ ìš”ì•½ í†µí•© (í‰ë©´í™”)
  const allDocumentLearnings = learningsPerDoc.flat();
  log(`[DEBUG] Total individual learnings generated: ${allDocumentLearnings.length}`);

  // 4. ë¹ˆ ë°ì´í„° ì²˜ë¦¬ - ê°œë³„ í•™ìŠµ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ì•ˆì „ì¥ì¹˜
  if (allDocumentLearnings.length === 0) {
    log(`[ERROR] No individual learnings generated for query: ${query}`);
    return {
      learnings: [`## ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\n\nê²€ìƒ‰ì–´ "${query}"ì— ëŒ€í•œ ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ê²€ìƒ‰ ë°©ì‹ì„ ì‹œë„í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.`],
      followUpQuestions: [`${query}ì™€ ê´€ë ¨ëœ ë‹¤ë¥¸ ê²€ìƒ‰ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?`],
    };
  }

  // 5. ì „ì²´ í†µí•© ìš”ì•½ ìƒì„± (ì¤„ê¸€ í˜•ì‹)
  const res = await generateObject({
    model: getResearchModel(),
    abortSignal: AbortSignal.timeout(90_000), // ìƒì„¸í•œ ì‘ì„±ì„ ìœ„í•´ íƒ€ì„ì•„ì›ƒ ì¦ê°€
    system: systemPrompt(),
    prompt: trimPrompt(
      `Using the <individual_learnings> provided, produce an **English narrative** for the search query â€œ${query}.â€

### Objectives 
1. **Preserve original wording and nuance** wherever possible.  
   - Keep source sentences intact unless minor grammatical fixes are required for readability.  
   - Maintain the original logical order of the information.

2. Remove only obvious noise (ads, navigation text, duplicate headers).  
   Everything elseâ€”including niche details, statistics, and specialised terminologyâ€”should remain.

3. Organise the text with ## sub-headings that reflect each major topic, but **do not convert content into tables or bullet lists**.  
   Use continuous prose; each paragraph may freely quote or embed source sentences.

4. Aim for  at least **1,000+ words** so the final piece is rich and comprehensive.\n\nEnd the document with a short section titled Follow-Up Questions that lists ${numFollowUpQuestions} further questions to deepen future research.\n\n<individual_learnings>\n${allDocumentLearnings.map(learning => `- ${learning}`).join('\n')}\n</individual_learnings>`,
    ),
    schema: z.object({
      detailedSummary: z.string().describe('A detailed, structured summary in Korean with appropriate section headers (## format). Each section should contain comprehensive narrative content while preserving original information details.'),
      followUpQuestions: z
        .array(z.string())
        .describe(
          `List of follow-up questions to research the topic further, max of ${numFollowUpQuestions}`,
        ),
    }),
  });

  // ìƒì„±ëœ ìƒì„¸ ìš”ì•½ë¬¸ì„ ë°°ì—´ì— ë‹´ì•„ ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ í•¨
  const learnings = [res.object.detailedSummary];
  log(`[DEBUG] Created a detailed summary and ${res.object.followUpQuestions.length} questions`);
  
  return {
    learnings,
    followUpQuestions: res.object.followUpQuestions,
  };
}

export async function writeFinalReport({
  prompt,
  learnings,
  visitedUrls,
}: {
  prompt: string;
  learnings: string[];
  visitedUrls: string[];
}) {
  const learningsString = learnings
    .map(learning => `<learning>\n${learning}\n</learning>`)
    .join('\n');

  const res = await generateObject({
    model: getReportModel(),
    system: systemPrompt(),
    prompt: trimPrompt(
      `ì—°êµ¬ì—ì„œ ì–»ì€ ëª¨ë“  í•™ìŠµ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì£¼ì œì— ë§ëŠ” ì‹¬ì¸µ ê¸°ìˆ  ì „ëµ ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

You are an AI Research Agent assigned to prepare an in-depth, structured report to support high-level business decision-making for a specific IT solution.

The report must synthesize all research findings into original, evidence-based insight (20+ pages) and follow this structure:

1. **Executive Summary & Strategic Implications**
2. **Market Opportunity & Demand Analysis**
3. **Competitive Positioning & Strategic Differentiators**
4. **Technology Fit & Architectural Feasibility**
5. **Go-to-Market & Adoption Strategy**
6. **Risk Exposure & Mitigation Plan**
7. **Business Case & ROI Scenarios**

Strictly follow these writing guidelines for every section:
- Use Markdown # headings; ### for sub-sections.
- â‰¥ 2-3 full paragraphs per sub-section, each â‰¥ 8 well-formed sentences.
- Provide quantitative data, benchmarks, tables, and bullet lists where relevant.
- End each main section with a **â€œBusiness Implicationâ€** paragraph explaining
  how the findings affect market entry, differentiation, or investment logic.
- In *Competitive Positioning* include deep-dive company profiles **and** a gap /
  white-space analysis that reveals unmet opportunities.
- Maintain executive-level perspective: focus on insights that influence high-level
  business decisions; avoid purely academic detail

All information must be synthesized into original insightâ€”not copied or paraphrased. Avoid brief summaries as the main format. Every section must contain rich explanation and thoughtful discussion.
**Please write your response in Korean.**

Here is the result of All learnings. <learnings>\n${learningsString}\n</learnings>
`,
    ),
    schema: z.object({
      reportMarkdown: z.string().describe('Comprehensive, detailed strategic report on the topic in Markdown format with extensive analysis and insights'),
    }),
  });

  // Append the visited URLs section to the report
  const urlsSection = `\n\n## Sources\n\n${visitedUrls.map(url => `- ${url}`).join('\n')}`;
  return res.object.reportMarkdown + urlsSection;
}

export async function writeFinalAnswer({
  prompt,
  learnings,
}: {
  prompt: string;
  learnings: string[];
}) {
  const learningsString = learnings
    .map(learning => `<learning>\n${learning}\n</learning>`)
    .join('\n');

  const res = await generateObject({
    model: getAnswerModel(),
    system: systemPrompt(),
    prompt: trimPrompt(
      `Given the following prompt from the user, write a final answer on the topic using the learnings from research. Follow the format specified in the prompt. Do not yap or babble or include any other text than the answer besides the format specified in the prompt. Keep the answer as concise as possible - usually it should be just a few words or maximum a sentence. Try to follow the format specified in the prompt (for example, if the prompt is using Latex, the answer should be in Latex. If the prompt gives multiple answer choices, the answer should be one of the choices).\n\n<prompt>${prompt}</prompt>\n\nHere are all the learnings from research on the topic that you can use to help answer the prompt:\n\n<learnings>\n${learningsString}\n</learnings>`,
    ),
    schema: z.object({
      exactAnswer: z
        .string()
        .describe('The final answer, make it short and concise, just the answer, no other text'),
    }),
  });

  return res.object.exactAnswer;
}

export async function deepResearch({
  solutionContext, // ì´ ë¶€ë¶„ì´ solutionContextë¡œ ì‚¬ìš©ë¨
  query, // ì´ ë¶€ë¶„ì´ initialQueryë¡œ ì‚¬ìš©ë¨
  breadth,
  depth,
  learnings = [], // ì´ì „ depthì—ì„œ ëˆ„ì ëœ learnings
  visitedUrls = [], // ì´ì „ depthì—ì„œ ëˆ„ì ëœ visitedUrls
  onProgress,
  initialQuery, // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ë³´ì¡´ìš©
  originalDepth, // ìµœì´ˆ depth ê°’ ë³´ì¡´ìš©
  parentDimension, // 1ì°¨ ê²€ìƒ‰ì˜ ë””ë©˜ì…˜ ì •ë³´
}: {
  solutionContext?: string;
  query: string;
  breadth: number;
  depth: number;
  learnings?: string[];
  visitedUrls?: string[];
  onProgress?: (progress: ResearchProgress) => void;
  initialQuery?: string; // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ë³´ì¡´ìš©
  originalDepth?: number; // ìµœì´ˆ depth ê°’ ë³´ì¡´ìš©
  parentDimension?: string; // 1ì°¨ ê²€ìƒ‰ì˜ ë””ë©˜ì…˜ ì •ë³´
}): Promise<ResearchResult> {
  // originalDepthê°€ ì—†ìœ¼ë©´ í˜„ì¬ depthë¥¼ ìµœì´ˆ depthë¡œ ì„¤ì •
  const maxDepth = originalDepth || depth;
  
  const progress: ResearchProgress = {
    currentDepth: depth,
    totalDepth: maxDepth, // ìµœì´ˆ depth ê°’ ì‚¬ìš©
    currentBreadth: breadth,
    totalBreadth: breadth,
    totalQueries: 0,
    completedQueries: 0,
  };

  const reportProgress = (update: Partial<ResearchProgress>) => {
    Object.assign(progress, update);
    onProgress?.(progress);
  };

  // 1ì°¨ ê²€ìƒ‰ì€ useDimensions: true, 2ì°¨ ê²€ìƒ‰(depth < maxDepth)ì€ useDimensions: false
  const isInitialSearch = depth === maxDepth;
  
  const serpQueries = await generateSerpQueries({
    query,
    learnings,
    numQueries: breadth,
    useDimensions: isInitialSearch, // 2ì°¨ ê²€ìƒ‰ë¶€í„°ëŠ” ë””ë©˜ì…˜ ì‚¬ìš© ì•ˆ í•¨
    parentDimension, // parentDimension ì „ë‹¬ ì¶”ê°€
  });

  reportProgress({
    totalQueries: serpQueries.length,
    currentQuery: serpQueries[0]?.query,
  });

  const limit = pLimit(ConcurrencyLimit);

  // í•˜ìœ„ ë³´ê³ ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± ë¡œì§ ì¶”ê°€
  // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë”ëª… ìƒì„± (ì¬ê·€ í˜¸ì¶œ ì‹œì—ë„ ë™ì¼í•œ í´ë” ì‚¬ìš©)
  const queryForFolder = initialQuery || query;
  let safeInitialQuery = queryForFolder.replace(/[\/\?%\*:\|"<>\.]/g, '').replace(/\s+/g, '_');
  // Windowsì˜ MAX_PATH ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë””ë ‰í† ë¦¬ ì´ë¦„ ë¶€ë¶„ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
  // MAX_PATHëŠ” ì•½ 260ìì´ë©°, ê¸°ë³¸ ê²½ë¡œ, 'report', '<ê²€ìƒ‰ì–´>', 'Middle', ì¼ë°˜ì ì¸ íŒŒì¼ ì´ë¦„ì„ ì œì™¸í•˜ë©´
  // ì´ ë¶€ë¶„ì— ì•½ 150ì ì •ë„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  const MAX_DIR_PART_LENGTH = 100;
  if (safeInitialQuery.length > MAX_DIR_PART_LENGTH) {
    safeInitialQuery = safeInitialQuery.substring(0, MAX_DIR_PART_LENGTH);
    // ì˜ë¼ë‚¸ í›„ ë§ˆì§€ë§‰ì— ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” '_' ë¬¸ìë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
    safeInitialQuery = safeInitialQuery.replace(/_+$/, '');
  }
  
  const getFolderName = (currentDepth: number, maxDepth: number) => {
    if (currentDepth === maxDepth) {
      return 'Middle'; // 1ì°¨ ê²€ìƒ‰ (ìµœì´ˆ depth)
    } else {
      const followUpLevel = maxDepth - currentDepth;
      return `FollowUp_${followUpLevel}`; // 2ì°¨: FollowUp_1, 3ì°¨: FollowUp_2, ...
    }
  };
  
  const folderName = getFolderName(depth, maxDepth);
  const subReportDir = path.join('report', safeInitialQuery, folderName);
  await fs.mkdir(subReportDir, { recursive: true });

  // ğŸ” ê²€ìƒ‰ì–´ ë©”íƒ€ë°ì´í„° ì €ì¥ (ê¸°ì¡´ ì½”ë“œì— ì˜í–¥ ì—†ìŒ)
  await saveSearchQueries(subReportDir, serpQueries, depth, maxDepth, parentDimension);

  const results = await Promise.all(
    serpQueries.map(serpQuery =>
      limit(async () => {
        try {
          const crawler = await getCrawlerInstance();
          const crawlerType = process.env.CRAWLER_TYPE || 'perplexity';

          let result: SearchResult;
          if (crawlerType === 'perplexity') {
            // perplexity searchê°€ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬
            result = await crawler.search(serpQuery.query);
          } else if (crawlerType === 'firecrawl') {
            // firecrawl searchê°€ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬ (perplexityì™€ ë™ì¼)
            result = await crawler.search(serpQuery.query, {
              timeout: 30000,
              limit: 10,
              scrapeOptions: { 
                formats: ['markdown'],
                // maxTokens ì œê±° - Firecrawl v1 APIì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
              },
            });
          } else {
            // ê¸°ì¡´ í¬ë¡¤ëŸ¬ë“¤ (google, crawl4ai)ì€ search í›„ crawl í•„ìš”
            result = await crawler.search(serpQuery.query, {
              timeout: 30000, // íƒ€ì„ì•„ì›ƒ ì‹œê°„ ì¦ê°€
              limit: 10, // ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ë¥¼ 10ê°œë¡œ ì œí•œ
              scrapeOptions: { 
                formats: ['markdown'], 
                maxTokens: 100000, // í˜ì´ì§€ë‹¹ ìµœëŒ€ í† í° ìˆ˜ ì¦ê°€
              },
            });
          }

          // Collect URLs from this search
          const newUrls = compact(result.data.map(item => item.url));
          // breadth ê³„ì‚° ì›ë˜ëŒ€ë¡œ ìœ ì§€ (2ì°¨ ê²€ìƒ‰ë„ breadth ê°’ ì‚¬ìš©)
          const newBreadth = Math.ceil(breadth / 3);
          const newDepth = depth - 1;

          const newLearnings = await processSerpResult({
            // ê¸°ì¡´ processSerpResult í˜¸ì¶œ ë¶€ë¶„ ìœ ì§€
            query: serpQuery.query,
            result,
            numLearnings: 10, // ê¸°ë³¸ê°’ 5ì—ì„œ 10ìœ¼ë¡œ ì¦ê°€ (ë‚´ìš© ë³´ì¡´ì„ ìœ„í•´)
            numFollowUpQuestions: newBreadth,
          });
          const allLearnings = [...learnings, ...newLearnings.learnings];
          const allUrls = [...visitedUrls, ...newUrls];

          // í•˜ìœ„ ì£¼ì œ ê²°ê³¼ íŒŒì¼ ì €ì¥ ë¡œì§ ì¶”ê°€
          if (newLearnings.learnings.length > 0) {
            // 1ì°¨ ê²€ìƒ‰ì¸ì§€ í™•ì¸ (originalDepthì™€ í˜„ì¬ depth ë¹„êµ)
            const isInitialSearch = depth === (originalDepth || depth);
            
            // ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            if ('dimension' in serpQuery && serpQuery.dimension === 'Core Technology & Architecture') {
              console.log('\n=== CoreTechnologyArchitecture ë””ë²„ê¹… ===');
              console.log('followUpQuestions:', newLearnings.followUpQuestions);
              console.log('learnings length:', newLearnings.learnings.length);
              console.log('newDepth:', newDepth);
              console.log('newBreadth:', newBreadth);
            }
            
            // íŒŒì¼ëª… ìƒì„± ë¡œì§ ê°œì„ : ë””ë©˜ì…˜ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…
            let fileNamePrefix: string;
            
            if (isInitialSearch && 'dimension' in serpQuery && typeof serpQuery.dimension === 'string') {
              // 1ì°¨ ê²€ìƒ‰: ë””ë©˜ì…˜ ê¸°ë°˜ íŒŒì¼ëª…
              fileNamePrefix = serpQuery.dimension.replace(/[^a-zA-Z0-9_\-ê°€-í£]/g, '').replace(/\s+/g, '_');
            } else if (parentDimension) {
              // 2ì°¨ ê²€ìƒ‰: ë¶€ëª¨ ë””ë©˜ì…˜ + ì¸ë±ìŠ¤ ê¸°ë°˜ íŒŒì¼ëª…
              const dimensionSlug = parentDimension.replace(/[^a-zA-Z0-9_\-ê°€-í£]/g, '').replace(/\s+/g, '_');
              // ê°„ë‹¨í•œ ì¸ë±ìŠ¤ ìƒì„± (í˜„ì¬ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ ì„± ë³´ì¥)
              const timeIndex = Date.now().toString().slice(-4);
              const queryIndex = progress.completedQueries + 1;
              fileNamePrefix = `${dimensionSlug}_${queryIndex}_${timeIndex}`;
            } else {
              // ê¸°ë³¸ fallback
              fileNamePrefix = `Query_${progress.completedQueries + 1}`;
            }
            
            const subReportFileName = `${fileNamePrefix}.md`;
            const subReportFilePath = path.join(subReportDir, subReportFileName);
            try {
              await fs.writeFile(subReportFilePath, newLearnings.learnings.join('\n\n---\n\n'), 'utf-8');
              log(`Sub-report saved: ${subReportFilePath}`);
            } catch (writeError) {
              console.error(`Error writing sub-report ${subReportFilePath}:`, writeError);
            }
          }

          if (newDepth > 0) {
            log(`Researching deeper, breadth: ${newBreadth}, depth: ${newDepth}`);

            reportProgress({
              currentDepth: newDepth,
              currentBreadth: newBreadth,
              completedQueries: progress.completedQueries + 1,
              currentQuery: serpQuery.query,
            });

            const nextQuery = `
            Previous research goal: ${serpQuery.researchGoal}
            Follow-up research directions: ${newLearnings.followUpQuestions.map(q => `\n${q}`).join('')}
          `.trim();

            return await deepResearch({
              query: nextQuery,
              breadth: Math.ceil(breadth / 3),
              depth: depth - 1,
              learnings: allLearnings,
              visitedUrls: allUrls,
              onProgress,
              solutionContext, // ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ solutionContext ì „ë‹¬
              initialQuery: initialQuery || query, // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ì „ë‹¬
              originalDepth: maxDepth, // ìµœì´ˆ depth ê°’ ì „ë‹¬
              parentDimension: parentDimension || 
                ('dimension' in serpQuery && typeof serpQuery.dimension === 'string' 
                  ? serpQuery.dimension 
                  : undefined), // ìƒìœ„ ë””ë©˜ì…˜ì´ ìˆìœ¼ë©´ ìœ ì§€, ì—†ìœ¼ë©´ serpQuery.dimension ì‚¬ìš©
            });
          } else {
            reportProgress({
              currentDepth: 0,
              completedQueries: progress.completedQueries + 1,
              currentQuery: serpQuery.query,
            });
            return {
              learnings: allLearnings,
              visitedUrls: allUrls,
            };
          }
        } catch (e: any) {
          console.error(`[CASCADE DEBUG] Full error for query "${serpQuery.query}":`, JSON.stringify(e, null, 2));
          if (e.message && e.message.includes('Timeout')) {
            log(`Timeout error running query: ${serpQuery.query}: `, e);
          } else {
            log(`Error running query: ${serpQuery.query}: `, e);
          }
          return {
            learnings: [],
            visitedUrls: [],
          };
        }
      }),
    ),
  );

  return {
    learnings: [...new Set(results.flatMap(r => r.learnings))],
    visitedUrls: [...new Set(results.flatMap(r => r.visitedUrls))],
  };
}
