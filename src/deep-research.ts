// í¬ë¡¤ëŸ¬ ì¸í„°í˜ì´ìŠ¤ import
import { Crawler, SearchResult } from './crawlers';
import { generateObject } from 'ai';
import { compact } from 'lodash-es';
import pLimit from 'p-limit';
import { z } from 'zod';
import * as path from 'path';
import * as fs from 'node:fs/promises';
import { generateComprehensiveReport } from './comprehensive-report-generator';

import { getModel, getQueryModel, getResearchModel, getReportModel, getAnswerModel, trimPrompt } from './ai/providers';
import { systemPrompt } from './prompt'; // systemPrompt import

function log(...args: any[]) {
  console.log(...args);
}

/**
 * ê²€ìƒ‰ì–´ ë©”íƒ€ë°ì´í„°ë¥¼ í†µí•© êµ¬ì¡°ì²´ë¡œ ì €ì¥
 * 1ì°¨/2ì°¨ ê²€ìƒ‰ì„ ëª¨ë‘ í•˜ë‚˜ì˜ íŒŒì¼ì— ëˆ„ì  ì €ì¥
 */
async function saveSearchQueries(
  folderPath: string, 
  queries: any[], 
  depth: number,
  maxDepth: number,
  parentDimension?: string
) {
  try {
    // í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™ (report/í”„ë¡œì íŠ¸ëª…/)
    const projectRootPath = path.dirname(folderPath);
    const queryFilePath = path.join(projectRootPath, 'queries.json');
    const projectName = path.basename(projectRootPath);
    
    // ê¸°ì¡´ íŒŒì¼ ì½ê¸° (ì—†ìœ¼ë©´ ì´ˆê¸° êµ¬ì¡° ìƒì„±)
    let searchData: any;
    try {
      const existingData = await fs.readFile(queryFilePath, 'utf-8');
      searchData = JSON.parse(existingData);
    } catch {
      // íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ êµ¬ì¡° ìƒì„±
      searchData = {
        projectInfo: {
          projectName,
          startTime: new Date().toISOString(),
          totalDepth: maxDepth
        },
        searchHistory: {
          "1ì°¨ê²€ìƒ‰": null,
          "2ì°¨ê²€ìƒ‰": {}
        }
      };
    }
    
    const currentTime = new Date().toISOString();
    const folderName = depth === maxDepth ? 'Middle' : `FollowUp_${maxDepth - depth}`;
    const isInitialSearch = depth === maxDepth;
    
    const querySection = {
      depth,
      folderName,
      timestamp: currentTime,
      parentDimension: parentDimension || null,
      totalQueries: queries.length,
      queries: queries.map((q, index) => ({
        index: index + 1,
        query: q.query,
        dimension: q.dimension || null,
        researchGoal: q.researchGoal || null,
        expectedResultFile: null
      }))
    };
    
    if (isInitialSearch) {
      // 1ì°¨ ê²€ìƒ‰: ì „ì²´ ì €ì¥
      searchData.searchHistory["1ì°¨ê²€ìƒ‰"] = querySection;
    } else {
      // 2ì°¨ ê²€ìƒ‰: í•´ë‹¹ dimensionì—ë§Œ ì €ì¥
      const dimensionKey = parentDimension
        ? parentDimension.replace(/[^a-zA-Z0-9ê°€-í£]/g, '').replace(/\s+/g, '')
        : `Unknown_${Date.now()}`;
      
      searchData.searchHistory["2ì°¨ê²€ìƒ‰"][dimensionKey] = querySection;
    }
    
    // íŒŒì¼ ì €ì¥
    await fs.writeFile(queryFilePath, JSON.stringify(searchData, null, 2), 'utf-8');
    log(`âœ… Search queries saved: ${queryFilePath} (${isInitialSearch ? '1ì°¨ê²€ìƒ‰' : '2ì°¨ê²€ìƒ‰-' + parentDimension})`);
    
  } catch (error) {
    console.warn(`âš ï¸ Failed to save search queries: ${error}`);
  }
}

export type ResearchProgress = {
  currentDepth: number;
  totalDepth: number;
  currentBreadth: number;
  totalBreadth: number;
  currentQuery?: string;
  totalQueries: number;
  completedQueries: number;
};

export type ResearchResult = {
  learnings: string[];
  visitedUrls: string[];
  comprehensiveReport?: string; // ìƒˆë¡œ ì¶”ê°€
  reportPaths?: string[]; // ìƒˆë¡œ ì¶”ê°€
};

// increase this if you have higher API rate limits
const ConcurrencyLimit = Number(process.env.CRAWLER_CONCURRENCY) || 3;

// í¬ë¡¤ëŸ¬ íƒ€ì… (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
const crawlerType = process.env.CRAWLER_TYPE || 'crawl4ai';

// ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
console.log('í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì •ë³´:');
console.log('í¬ë¡¤ëŸ¬ íƒ€ì…:', crawlerType);

// í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ (lazy loading)
let crawlerInstance: Crawler | null = null;

// í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
async function getCrawlerInstance(): Promise<Crawler> {
  if (!crawlerInstance) {
    const { getCrawler } = await import('./crawlers');
    crawlerInstance = await getCrawler(crawlerType as any);
  }
  return crawlerInstance;
}

// take en user query, return a list of SERP queries, ì¼ë‹¨ numqueriesëŠ” 3ê°œì—ì„œ ë³€ê²½í•¨. 
export async function generateSerpQueries({
  query,
  numQueries = 3,
  learnings,
  useDimensions = true,
  parentDimension, // ë””ë©˜ì…˜ ì‚¬ìš© ì—¬ë¶€ (1ì°¨ ê²€ìƒ‰ì—ì„œëŠ” true, 2ì°¨ ê²€ìƒ‰ì—ì„œëŠ” false)
}: {
  query: string;
  numQueries?: number;
  // optional, if provided, the research will continue from the last learning
  learnings?: string[];
  useDimensions?: boolean;
  parentDimension?: string;
}) {
    // âœ… 1. ë¨¼ì € ê²€ìƒ‰ ëª¨ë“œ íŒë³„
  const isFirstLevel = !parentDimension || parentDimension.trim() === '';

  // âœ… 2. í”„ë¡¬í”„íŠ¸ ë¸”ë¡ ì •ì˜
  const firstLevelPrompt = `You are conducting an initial broad search to explore the topic across the 6 core strategic dimensions listed below.

    Generate 6 search queries in English â€” one for each of the following business-critical strategic dimensions:
    TOPIC ANCHOR
    Each query MUST:
      â€¢ Add only dimension-specific terms that relate directly to this topic.
      â€¢ Exclude unrelated or broader synonyms.

    Solution Overview & Market Analysis
    â€” solution scope and functional definition, global and Korean market size with growth rates (TAM/SAM/CAGR), primary value proposition, and major application areas across industries

    Customer Intelligence & Business Case  
    â€” target customer segmentation taxonomy (industry, size, geography), quantified pain-point severity and business impact, ROI case studies and success stories, plus customer willingness-to-pay indicators

    Technology Landscape & Trends
    â€” latest technology trends and emerging innovations for 2024-2025, core technology stack patterns and architecture options, technology maturity assessment, and open-source ecosystem plus standardization status

    Competitive Technology Analysis
    â€” major competitors and market leaders with technology positioning, competitive feature comparison and technical capabilities, vendor ecosystem and partnership landscape, plus differentiation opportunities and competitive advantages

    Technology Implementation & ROI
    â€” proven implementation methodologies and best practices, development challenges and technical risk mitigation, quantified cost-benefit analysis with ROI calculation examples, and scalability plus performance benchmark data

    Risk & Regulatory Analysis
    â€” technical limitations and implementation risks, current and upcoming regulatory requirements plus compliance standards, security and privacy concerns with data protection, and market entry barriers plus adoption challenges

    Each query should view the userâ€™s topic through the lens of the corresponding strategic dimension.`; // (ìœ„ ë‚´ìš© ê·¸ëŒ€ë¡œ)
  const secondLevelPrompt = `You are conducting a focused second-level search on the dimension: **${parentDimension}**
    Generate ${numQueries} follow-up search queries in English (3â€“10 words each) that:
    - Generate search queries by selecting one sub-topic under each **"${parentDimension}"** below:
    Solution Overview
    Solution Overview & Market Analysis:
    1-1. Solution Definition & Core Features (functionality, technical specs, capabilities)
    1-2. Market Size & Growth Dynamics (TAM/SAM/CAGR, growth drivers, geographic trends)
    1-3. Value Proposition & Competitive Position (differentiation, market positioning)
    1-4. Industry Applications & Use Cases (verticals, scenarios, adoption patterns)

    Customer Intelligence & Business Case:
    2-1. Customer Segmentation & Personas (target segments, decision-makers, buyer journey)
    2-2. Pain Points & Business Impact (problems, severity metrics, current costs)
    2-3. ROI & Business Value (quantified ROI, payback periods, efficiency gains)
    2-4. Success Stories & Case Studies (testimonials, reference customers, proven results)

    Technology Landscape & Trends:
    3-1. Latest Technology Trends & Roadmap (2024-2025 trends, emerging innovations)
    3-2. Technology Stack & Architecture (components, patterns, infrastructure requirements)
    3-3. Technology Maturity & Standards (readiness level, industry standards, protocols)
    3-4. Ecosystem & Open Source (developer community, open-source projects, integrations)

    Competitive Technology Analysis:
    4-1. Major Competitors & Market Leaders (key players, market share, positioning)
    4-2. Technology Feature Comparison (feature comparison, technical capabilities, benchmarks)
    4-3. Vendor Ecosystem & Partnerships (partner networks, integrations, alliances)
    4-4. Differentiation & Competitive Advantage (USPs, market gaps, opportunities)

    Technology Implementation & ROI:
    5-1. Implementation Methodology & Best Practices (deployment approaches, frameworks)
    5-2. Development Challenges & Solutions (obstacles, risk mitigation, problem-solving)
    5-3. Cost-Benefit Analysis & ROI Calculation (costs, TCO, ROI methods, examples)
    5-4. Performance & Scalability Benchmarks (metrics, optimization, capacity planning)

    Risk & Regulatory Analysis:
    6-1. Technical Risks & Limitations (technology limits, failure modes, reliability)
    6-2. Regulatory & Compliance Requirements (regulations, standards, certifications)
    6-3. Security & Privacy Concerns (cybersecurity, data privacy, protection measures)
    6-4. Market Barriers & Adoption Challenges (entry barriers, adoption obstacles, change management)
    `; // (ìœ„ ë‚´ìš© ê·¸ëŒ€ë¡œ)
  const res = await generateObject({
    model: getQueryModel(),
    system: systemPrompt(),
    prompt: `Given the user query, extract key concepts and generate simple, broad search queries that cover the topic comprehensively. 
User Query: ${query}
Parent Dimension: ${parentDimension}

Step 1 â€” Determine the search mode:

- If **parentDimension is not provided** (i.e., it is null or empty), this is an **initial broad exploration** step.  
  You should generate **one query per strategic dimension** to explore the topic from all relevant angles.
  Please Be sure to stay focused on the user's topic. 
  
- If **parentDimension is provided**, it indicates the current research is a **follow-up deep dive** focused on that specific strategic dimension.  
  You should generate **multiple follow-up queries** targeting only that dimension.

Regardless of the mode, follow these principles:
- Always include the topic keyword from the original query in every generated search query.
- Keep each query short and simple (min 4 ~ max 12 words)
- Avoid redundancy â€” do not repeat similar queries or rephrase the same idea
- Ensure each query targets a **unique, meaningful aspect** of the topic

${(isFirstLevel ? firstLevelPrompt : secondLevelPrompt)}

${learnings ? 
  `\nPrevious research context:\n${learnings.slice(0, 2).join('\n\n')}\n\nBased on this context, generate queries that explore gaps or expand on key findings.` 
  : ''
}`,
    schema: z.object({
      queries: useDimensions 
        ? z.array(
            z.object({
              dimension: z.enum([
                "SolutionOverviewMarketAnalysis",
                "CustomerIntelligenceBusinessCase",
                "TechnologyLandscapeTrends",
                "CompetitiveTechnologyAnalysis",
                "TechnologyImplementationROI",
                "RiskRegulatoryAnalysis"
              ]).describe("The strategic dimension this query focuses on."),
              query: z.string().describe('Simple, broad search query (3-7 words, covering key aspects of the topic).'),
              researchGoal: z
                .string()
                .describe(
                  'Brief goal of this research query and what insights we expect to find.',
                ),
            })
          )
          .length(6) // 1ì°¨ ê²€ìƒ‰: 6ê°€ì§€ ë””ë©˜ì…˜ ê°•ì œ
          .describe(`List of 6 simple, broad SERP queries covering different strategic aspects.`)
        : z.array( // 2ì°¨ ê²€ìƒ‰: dimension í•„ë“œ ì™„ì „ ì œê±°
            z.object({
              query: z.string().describe('Simple follow-up search query (3-10 words).'),
              researchGoal: z.string().describe('Brief goal of this follow-up research.'),
            })
          )
          .min(1)
          .max(numQueries)
          .describe(`List of simple follow-up queries for deeper research.`),
    }),
  });
  log(`Created ${res.object.queries.length} queries`, res.object.queries);

  return res.object.queries.slice(0, numQueries);
}
// numLearnings, Fup Questions ëŠ” 2ê°œë¡œ ë³€ê²½
export async function processSerpResult({
  query,
  result,
  numLearnings = 3,
  numFollowUpQuestions = 3,
}: {
  query: string;
  result: SearchResult;
  numLearnings?: number;
  numFollowUpQuestions?: number;
}) {
  // 1. ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
  log(`[DEBUG] Processing SERP result for query: "${query}"`);
  log(`[DEBUG] Raw result data length: ${result.data?.length || 0}`);
  log(`[DEBUG] Raw result structure:`, JSON.stringify(result, null, 2).substring(0, 500) + '...');
  
  // 2. ê²€ìƒ‰ ê²°ê³¼(markdown) ì¶”ì¶œ - ìµœëŒ€ 5ê°œë¡œ ì œí•œ
  const contents = compact(result.data.map(item => item.markdown)).slice(0, 10);
  log(`[DEBUG] Extracted contents length: ${contents.length} (limited to 5)`);
  
  // ê° ì½˜í…ì¸ ì˜ ê¸¸ì´ë„ í™•ì¸
  contents.forEach((content, index) => {
    log(`[DEBUG] Content ${index + 1} length: ${content?.length || 0} chars`);
  });
  
  // ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ê²°ê³¼ ë°˜í™˜
  if (contents.length === 0) {
    log(`[ERROR] No contents found for query: ${query}`);
    log(`[ERROR] Available data fields:`, Object.keys(result.data[0] || {}));
    return {
      learnings: [],
      followUpQuestions: [],
    };
  }

  // 2. ê° ë¬¸ì„œë³„ ê°œë³„ ìš”ì•½ ìƒì„± - ë™ì‹œì„± ì œí•œìœ¼ë¡œ ìˆœì°¨ ì²˜ë¦¬
  log(`Generating individual summaries for ${contents.length} documents...`);
  const docLimit = pLimit(3); // ë¬¸ì„œ ì²˜ë¦¬ ë™ì‹œì„±ì„ 5ê°œë¡œ ì œí•œ
  const learningsPerDoc = await Promise.all(
    contents.map((content, index) =>
      docLimit(async () => {
        try {
          // ê° ë¬¸ì„œ ë‚´ìš© ê¸¸ì´ ì œí•œ - í† í° ìˆ˜ ì¶•ì†Œ
          const trimmedContent = trimPrompt(content, 50_000);
          
          // ê°œë³„ ë¬¸ì„œì— ëŒ€í•œ ìš”ì•½ ìƒì„±
          const docRes = await generateObject({
            model: getResearchModel(),
            abortSignal: AbortSignal.timeout(60_000), // íƒ€ì„ì•„ì›ƒ ì¦ê°€
            system: systemPrompt(),
            prompt: `Given the following content from a search result for the query "${query}", generate at least more than 10 key learnings from this specific document. Make the learnings precise, detailed and information-dense. Include any entities, metrics, numbers, or dates.

<content>
${trimmedContent}
</content>`,
            schema: z.object({
              documentLearnings: z.array(z.string()).describe('key learnings from this document'),
            }),
          });
          
          log(`Document ${index + 1}: Generated ${docRes.object.documentLearnings.length} learnings`);
          return docRes.object.documentLearnings;
        } catch (error) {
          log(`Error processing document ${index + 1}:`, error);
          return []; // ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë°°ì—´ ë°˜í™˜í•˜ì—¬ ê³„ì† ì§„í–‰
        }
      })
    ),
  );

  // 3. ëª¨ë“  ê°œë³„ ìš”ì•½ í†µí•© (í‰ë©´í™”)
  const allDocumentLearnings = learningsPerDoc.flat();
  log(`[DEBUG] Total individual learnings generated: ${allDocumentLearnings.length}`);

  // 4. ë¹ˆ ë°ì´í„° ì²˜ë¦¬ - ê°œë³„ í•™ìŠµ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ì•ˆì „ì¥ì¹˜
  if (allDocumentLearnings.length === 0) {
    log(`[ERROR] No individual learnings generated for query: ${query}`);
    return {
      learnings: [`## ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\n\nê²€ìƒ‰ì–´ "${query}"ì— ëŒ€í•œ ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ê²€ìƒ‰ ë°©ì‹ì„ ì‹œë„í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.`],
      followUpQuestions: [`${query}ì™€ ê´€ë ¨ëœ ë‹¤ë¥¸ ê²€ìƒ‰ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?`],
    };
  }

  // 5. ì „ì²´ í†µí•© ìš”ì•½ ìƒì„± (ì¤„ê¸€ í˜•ì‹)
  const res = await generateObject({
    model: getResearchModel(),
    abortSignal: AbortSignal.timeout(90_000), // ìƒì„¸í•œ ì‘ì„±ì„ ìœ„í•´ íƒ€ì„ì•„ì›ƒ ì¦ê°€
    system: systemPrompt(),
    prompt: trimPrompt(
      `Using only the sentences inside <individual_learnings>, write a coherent narrative for the search query: â€œ${query}.â€
      You are a precise information extractor. Your primary goal is accuracy over length.

      ## Core Principles
      1. **Only use information explicitly present in sources**
      2. **Never generate content beyond what sources provide**
      3. **Quality over quantity - shorter accurate content is better than longer fabricated content**
      4. **Output in English unless otherwise specified**

      ## Extraction Rules

      1. **Information Gathering**
        - Extract meaningful chunks of information (2-4 sentences) that form complete thoughts
        - Combine directly related facts from the same source into coherent paragraphs
        - Keep statistical data, quotes, and specific claims intact
        - Preserve important context that appears immediately around key facts

      2. **Expansion Guidelines** (ONLY when source material allows)
        - If sources contain explanations, include them
        - If sources show cause-effect relationships, preserve them
        - If sources provide examples or comparisons, retain them
        - DO NOT create explanations, examples, or relationships not in sources

      3. **Structure**
        - Use ## topical headings based on actual content available
        - Under each heading:
          * Group related information from sources
          * Maintain logical flow by ordering facts sensibly
          * If only 1-2 facts exist for a topic, that's acceptable
        - Skip sections entirely if insufficient information exists

      4. **Length Management**
        - Target length: As long as source material supports (typically 600-1500 words)
        - Each section: Include what sources provide, no minimum requirement
        - If a topic has limited information, keep it brief
        - Never pad content to meet length targets

      5. **Anti-Hallucination Safeguards**
        - Before writing each sentence, verify it exists in sources
        - Use these markers when needed:
          * "[ì •ë³´ ì—†ìŒ]" - when expected information is missing
          * "[ì¶”ê°€ ì •ë³´ ë¶€ì¡±]" - when a topic seems incomplete
          * "[ì›ë¬¸ ì°¸ì¡° í•„ìš”]" - when sources hint at more detail not provided
        - If unsure whether information is from sources or inference, exclude it

      6. **Citation Rules** 
        1. Format â€“ Append citations as (Source: â€¦) or (Sources: â€¦, â€¦) after each fact-bearing sentence or paragraph.
        2. Acceptable source strings MUST satisfy at least one of the following:
            a) Widely recognized organisation, company, or government body
              e.g. Gartner, OECD, Microsoft
            b) Established journal, conference, or media outlet with optional year
              e.g. Nature 2024, IEEE ICCV 2023, MIT Technology Review
            c) Clear top-level domain of a reputable entity
              e.g. who.int, nasa.gov, ft.com, arxiv.org
        3. Special cases
            â€¢ arXiv cite as arXiv 2409.14858 2024 or arXiv.org â€“ Paper Title
            â€¢ Datasets or repositories include provider and dataset or repo name
              e.g. Kaggle â€“ Titanic Survival
        4. Disallowed identifiers omit the statement if only these are available:
            â€¢ Pure numbers or version strings 2409.14858v1, 2020, 1, 3, 5
            â€¢ Random codes or hashes SRTE57145DR, abc123
            â€¢ Generic words content, article, data, report
        5. Multiple sources separate with commas
            e.g. (Sources: Gartner 2024, arXiv 2409.14858 2024)
        6. Authority first favour facts backed by authoritative, reputable sources; ignore information that cannot be cited with a compliant identifier.

      7. **Information Density Check**
        - Acceptable to have short sections if sources are limited
        - Acceptable to have fewer sections if content is sparse
        - NOT acceptable to invent content to fill space
        - Focus on extracting all available valuable information rather than creating narrative

      ## Final Verification
      Before outputting, check each statement:
      - Can I point to the exact source sentence(s)?
      - Am I combining facts or creating new connections not in sources?
      - Would removing this statement lose source information?

      Remember: Your users prefer complete but concise information over lengthy but partially fabricated content..
      
      \\n\n<individual_learnings>\n${allDocumentLearnings.map(learning => `- ${learning}`).join('\n')}\n</individual_learnings>`,
    ),
    schema: z.object({
      detailedSummary: z.string().describe('A detailed, structured summary in Korean with appropriate section headers (## format). Each section should contain comprehensive narrative content while preserving original information details.'),
      followUpQuestions: z
        .array(z.string())
        .describe(
          `List of follow-up questions to research the topic further, max of ${numFollowUpQuestions}`,
        ),
    }),
  });

  // ìƒì„±ëœ ìƒì„¸ ìš”ì•½ë¬¸ì„ ë°°ì—´ì— ë‹´ì•„ ê¸°ì¡´ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ë„ë¡ í•¨
  const learnings = [res.object.detailedSummary];
  log(`[DEBUG] Created a detailed summary and ${res.object.followUpQuestions.length} questions`);
  
  return {
    learnings,
    followUpQuestions: res.object.followUpQuestions,
  };
}

export async function writeFinalAnswer({
  prompt,
  learnings,
}: {
  prompt: string;
  learnings: string[];
}) {
  const model = getAnswerModel();

  const learningsString = learnings
    .map(learning => `<learning>\n${learning}\n</learning>`)
    .join('\n');

  const { object: answer } = await generateObject({
    model,
    system: systemPrompt(),
    prompt: trimPrompt(
      `Please write a comprehensive, easy-to-read, and in-depth answer to the following question. The answer should be written in Korean and in markdown format. The answer should be based on the following learnings. You can also use your own knowledge to supplement the answer.\n\nQuestion: ${prompt}\n\nLearnings:\n${learningsString}`,
    ),
    schema: z.object({
      answer: z.string(),
    }),
  });

  return answer.answer;
}

export async function deepResearch({
  solutionContext, // ì´ ë¶€ë¶„ì´ solutionContextë¡œ ì‚¬ìš©ë¨
  query, // ì´ ë¶€ë¶„ì´ initialQueryë¡œ ì‚¬ìš©ë¨
  breadth,
  depth,
  learnings = [], // ì´ì „ depthì—ì„œ ëˆ„ì ëœ learnings
  visitedUrls = [], // ì´ì „ depthì—ì„œ ëˆ„ì ëœ visitedUrls
  onProgress,
  initialQuery, // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ë³´ì¡´ìš©
  originalDepth, // ìµœì´ˆ depth ê°’ ë³´ì¡´ìš©
  parentDimension, // 1ì°¨ ê²€ìƒ‰ì˜ ë””ë©˜ì…˜ ì •ë³´
}: {
  solutionContext?: string;
  query: string;
  breadth: number;
  depth: number;
  learnings?: string[];
  visitedUrls?: string[];
  onProgress?: (progress: ResearchProgress) => void;
  initialQuery?: string; // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ë³´ì¡´ìš©
  originalDepth?: number; // ìµœì´ˆ depth ê°’ ë³´ì¡´ìš©
  parentDimension?: string; // 1ì°¨ ê²€ìƒ‰ì˜ ë””ë©˜ì…˜ ì •ë³´
}): Promise<ResearchResult> {
  // originalDepthê°€ ì—†ìœ¼ë©´ í˜„ì¬ depthë¥¼ ìµœì´ˆ depthë¡œ ì„¤ì •
  const maxDepth = originalDepth || depth;
  
  const progress: ResearchProgress = {
    currentDepth: depth,
    totalDepth: maxDepth, // ìµœì´ˆ depth ê°’ ì‚¬ìš©
    currentBreadth: breadth,
    totalBreadth: breadth,
    totalQueries: 0,
    completedQueries: 0,
  };

  const reportProgress = (update: Partial<ResearchProgress>) => {
    Object.assign(progress, update);
    onProgress?.(progress);
  };

  // 1ì°¨ ê²€ìƒ‰ì€ useDimensions: true, 2ì°¨ ê²€ìƒ‰(depth < maxDepth)ì€ useDimensions: false
  const isInitialSearch = depth === maxDepth;
  
  const serpQueries = await generateSerpQueries({
    query,
    learnings,
    numQueries: breadth,
    useDimensions: isInitialSearch, // 2ì°¨ ê²€ìƒ‰ë¶€í„°ëŠ” ë””ë©˜ì…˜ ì‚¬ìš© ì•ˆ í•¨
    parentDimension, // parentDimension ì „ë‹¬ ì¶”ê°€
  });

  reportProgress({
    totalQueries: serpQueries.length,
    currentQuery: serpQueries[0]?.query,
  });

  const limit = pLimit(ConcurrencyLimit);

  // í•˜ìœ„ ë³´ê³ ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± ë¡œì§ ì¶”ê°€
  // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë”ëª… ìƒì„± (ì¬ê·€ í˜¸ì¶œ ì‹œì—ë„ ë™ì¼í•œ í´ë” ì‚¬ìš©)
  const queryForFolder = initialQuery || query;
  let safeInitialQuery = queryForFolder.replace(/[\/\?%\*:\|"<>\.]/g, '').replace(/\s+/g, '_');
  // Windowsì˜ MAX_PATH ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë””ë ‰í† ë¦¬ ì´ë¦„ ë¶€ë¶„ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
  // MAX_PATHëŠ” ì•½ 260ìì´ë©°, ê¸°ë³¸ ê²½ë¡œ, 'report', '<ê²€ìƒ‰ì–´>', 'Middle', ì¼ë°˜ì ì¸ íŒŒì¼ ì´ë¦„ì„ ì œì™¸í•˜ë©´
  // ì´ ë¶€ë¶„ì— ì•½ 150ì ì •ë„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  const MAX_DIR_PART_LENGTH = 100;
  if (safeInitialQuery.length > MAX_DIR_PART_LENGTH) {
    safeInitialQuery = safeInitialQuery.substring(0, MAX_DIR_PART_LENGTH);
    // ì˜ë¼ë‚¸ í›„ ë§ˆì§€ë§‰ì— ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” '_' ë¬¸ìë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
    safeInitialQuery = safeInitialQuery.replace(/_+$/, '');
  }
  
  const getFolderName = (currentDepth: number, maxDepth: number) => {
    if (currentDepth === maxDepth) {
      return 'Middle'; // 1ì°¨ ê²€ìƒ‰ (ìµœì´ˆ depth)
    } else {
      const followUpLevel = maxDepth - currentDepth;
      return `FollowUp_${followUpLevel}`; // 2ì°¨: FollowUp_1, 3ì°¨: FollowUp_2, ...
    }
  };
  
  const folderName = getFolderName(depth, maxDepth);
  const subReportDir = path.join('report', safeInitialQuery, folderName);
  await fs.mkdir(subReportDir, { recursive: true });

  // ğŸ” ê²€ìƒ‰ì–´ ë©”íƒ€ë°ì´í„° ì €ì¥ (ê¸°ì¡´ ì½”ë“œì— ì˜í–¥ ì—†ìŒ)
  await saveSearchQueries(subReportDir, serpQueries, depth, maxDepth, parentDimension);

  const results = await Promise.all(
    serpQueries.map(serpQuery =>
      limit(async () => {
        try {
          const crawler = await getCrawlerInstance();
          const crawlerType = process.env.CRAWLER_TYPE || 'perplexity';

          let result: SearchResult;
          if (crawlerType === 'perplexity') {
            // perplexity searchê°€ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬
            result = await crawler.search(serpQuery.query);
          } else if (crawlerType === 'firecrawl') {
            // firecrawl searchê°€ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬ (perplexityì™€ ë™ì¼)
            result = await crawler.search(serpQuery.query, {
              timeout: 30000,
              limit: 10,
              scrapeOptions: { 
                formats: ['markdown'],
                // maxTokens ì œê±° - Firecrawl v1 APIì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
              },
            });
          } else {
            // ê¸°ì¡´ í¬ë¡¤ëŸ¬ë“¤ (google, crawl4ai)ì€ search í›„ crawl í•„ìš”
            result = await crawler.search(serpQuery.query, {
              timeout: 30000, // íƒ€ì„ì•„ì›ƒ ì‹œê°„ ì¦ê°€
              limit: 10, // ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ë¥¼ 10ê°œë¡œ ì œí•œ
              scrapeOptions: { 
                formats: ['markdown'], 
                maxTokens: 100000, // í˜ì´ì§€ë‹¹ ìµœëŒ€ í† í° ìˆ˜ ì¦ê°€
              },
            });
          }

          // Collect URLs from this search
          const newUrls = compact(result.data.map(item => item.url));
          // breadth ê³„ì‚° ì›ë˜ëŒ€ë¡œ ìœ ì§€ (2ì°¨ ê²€ìƒ‰ë„ breadth ê°’ ì‚¬ìš©)
          const newBreadth = Math.ceil(breadth - 2);
          const newDepth = depth - 1;

          // ğŸ†• í¬ë¡¤ëŸ¬ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬ ì¶”ê°€
          let newLearnings;

          if (crawlerType === 'perplexity') {
            // PerplexityëŠ” ì´ë¯¸ ì™„ì„±ëœ ìš”ì•½ì´ë¯€ë¡œ processSerpResult ìŠ¤í‚µ
            const perplexityContent = result.data[0]?.markdown || '';
            
            // ê°„ë‹¨í•œ í›„ì† ì§ˆë¬¸ë§Œ ìƒì„±
            const followUpRes = await generateObject({
              model: getResearchModel(),
              system: systemPrompt(),
              prompt: `Based on this research about "${serpQuery.query}", generate ${newBreadth} follow-up research questions to explore deeper.

          Research content: ${trimPrompt(perplexityContent, 5000)}`,
              schema: z.object({
                followUpQuestions: z.array(z.string()).describe('Follow-up research questions'),
              }),
            });

            newLearnings = {
              learnings: [perplexityContent], // ğŸ”¥ Perplexity ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
              followUpQuestions: followUpRes.object.followUpQuestions,
            };
          } else {
            // ê¸°ì¡´ í¬ë¡¤ëŸ¬ë“¤ì€ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
            newLearnings = await processSerpResult({
              query: serpQuery.query,
              result,
              numLearnings: 10,
              numFollowUpQuestions: newBreadth,
            });
          }

          // í•¨ìˆ˜ ì§€ì›€
          /* const newLearnings = await processSerpResult({
            // ê¸°ì¡´ processSerpResult í˜¸ì¶œ ë¶€ë¶„ ìœ ì§€
            query: serpQuery.query,
            result,
            numLearnings: 10, // ê¸°ë³¸ê°’ 5ì—ì„œ 10ìœ¼ë¡œ ì¦ê°€ (ë‚´ìš© ë³´ì¡´ì„ ìœ„í•´)
            numFollowUpQuestions: newBreadth,
          }); */

          const allLearnings = [...learnings, ...newLearnings.learnings];
          const allUrls = [...visitedUrls, ...newUrls];

          // í•˜ìœ„ ì£¼ì œ ê²°ê³¼ íŒŒì¼ ì €ì¥ ë¡œì§ ì¶”ê°€
          if (newLearnings.learnings.length > 0) {
            // 1ì°¨ ê²€ìƒ‰ì¸ì§€ í™•ì¸ (originalDepthì™€ í˜„ì¬ depth ë¹„êµ)
            const isInitialSearch = depth === (originalDepth || depth);
            
            // ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            if ('dimension' in serpQuery && serpQuery.dimension === 'Core Technology & Architecture') {
              console.log('\n=== CoreTechnologyArchitecture ë””ë²„ê¹… ===');
              console.log('followUpQuestions:', newLearnings.followUpQuestions);
              console.log('learnings length:', newLearnings.learnings.length);
              console.log('newDepth:', newDepth);
              console.log('newBreadth:', newBreadth);
            }
            
            // íŒŒì¼ëª… ìƒì„± ë¡œì§ ê°œì„ : ë””ë©˜ì…˜ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…
            let fileNamePrefix: string;
            
            if (isInitialSearch && 'dimension' in serpQuery && typeof serpQuery.dimension === 'string') {
              // 1ì°¨ ê²€ìƒ‰: ë””ë©˜ì…˜ ê¸°ë°˜ íŒŒì¼ëª…
              fileNamePrefix = serpQuery.dimension.replace(/[^a-zA-Z0-9_\-ê°€-í£]/g, '').replace(/\s+/g, '_');
            } else if (parentDimension) {
              // 2ì°¨ ê²€ìƒ‰: ë¶€ëª¨ ë””ë©˜ì…˜ + ì¸ë±ìŠ¤ ê¸°ë°˜ íŒŒì¼ëª…
              const dimensionSlug = parentDimension.replace(/[^a-zA-Z0-9_\-ê°€-í£]/g, '').replace(/\s+/g, '_');
              // ê°„ë‹¨í•œ ì¸ë±ìŠ¤ ìƒì„± (í˜„ì¬ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ ì„± ë³´ì¥)
              const timeIndex = Date.now().toString().slice(-4);
              const queryIndex = progress.completedQueries + 1;
              fileNamePrefix = `${dimensionSlug}_${queryIndex}_${timeIndex}`;
            } else {
              // ê¸°ë³¸ fallback
              fileNamePrefix = `Query_${progress.completedQueries + 1}`;
            }
            
            const subReportFileName = `${fileNamePrefix}.md`;
            const subReportFilePath = path.join(subReportDir, subReportFileName);
            try {
              await fs.writeFile(subReportFilePath, newLearnings.learnings.join('\n\n---\n\n'), 'utf-8');
              log(`Sub-report saved: ${subReportFilePath}`);
            } catch (writeError) {
              console.error(`Error writing sub-report ${subReportFilePath}:`, writeError);
            }
          }

          if (newDepth > 0) {
            log(`Researching deeper, breadth: ${newBreadth}, depth: ${newDepth}`);

            reportProgress({
              currentDepth: newDepth,
              currentBreadth: newBreadth,
              completedQueries: progress.completedQueries + 1,
              currentQuery: serpQuery.query,
            });

            const nextQuery = `
            Previous research goal: ${serpQuery.researchGoal}
            Follow-up research directions: ${newLearnings.followUpQuestions.map(q => `\n${q}`).join('')}
          `.trim();

            return await deepResearch({
              query: nextQuery,
              breadth: Math.ceil(breadth - 2),
              depth: depth - 1,
              learnings: allLearnings,
              visitedUrls: allUrls,
              onProgress,
              solutionContext, // ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ solutionContext ì „ë‹¬
              initialQuery: initialQuery || query, // ìµœì´ˆ ì‚¬ìš©ì ì¿¼ë¦¬ ì „ë‹¬
              originalDepth: maxDepth, // ìµœì´ˆ depth ê°’ ì „ë‹¬
              parentDimension: parentDimension || 
                ('dimension' in serpQuery && typeof serpQuery.dimension === 'string' 
                  ? serpQuery.dimension 
                  : undefined), // ìƒìœ„ ë””ë©˜ì…˜ì´ ìˆìœ¼ë©´ ìœ ì§€, ì—†ìœ¼ë©´ serpQuery.dimension ì‚¬ìš©
            });
          } else {
            reportProgress({
              currentDepth: 0,
              completedQueries: progress.completedQueries + 1,
              currentQuery: serpQuery.query,
            });
            return {
              learnings: allLearnings,
              visitedUrls: allUrls,
            };
          }
        } catch (e: any) {
          console.error(`[CASCADE DEBUG] Full error for query "${serpQuery.query}":`, JSON.stringify(e, null, 2));
          if (e.message && e.message.includes('Timeout')) {
            log(`Timeout error running query: ${serpQuery.query}: `, e);
          } else {
            log(`Error running query: ${serpQuery.query}: `, e);
          }
          return {
            learnings: [],
            visitedUrls: [],
          };
        }
      }),
    ),
  );

  const finalResult: ResearchResult = {
    learnings: [...new Set(results.flatMap(r => r.learnings))],
    visitedUrls: [...new Set(results.flatMap(r => r.visitedUrls))],
  };

  // ğŸ†• ìµœì´ˆ ê¹Šì´ì—ì„œë§Œ comprehensive report ìƒì„±
  if (depth === (originalDepth || depth)) {
    try {
      console.log('ğŸ¯ Generating comprehensive report...');
      const safeInitialQuery = (initialQuery || query).replace(/[\/\?%\*:\|"<>\.]/g, '').replace(/\s+/g, '_');

      const comprehensiveResult = await generateComprehensiveReport({
        prompt: initialQuery || query,
        learnings: finalResult.learnings,
        visitedUrls: finalResult.visitedUrls,
        options: {
          includeDimensionReports: true,
          includeConsolidatedReport: true,
          includeStrategicReport: true,
          outputDirectory: path.join('report', safeInitialQuery)
        }
      });

      finalResult.comprehensiveReport = comprehensiveResult.strategicReport || comprehensiveResult.consolidatedReport;
      finalResult.reportPaths = comprehensiveResult.reportPaths;

    } catch (error) {
      console.error('âš ï¸ Comprehensive report generation failed:', error);
      // ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ê²°ê³¼ëŠ” ë°˜í™˜
    }
  }

  return finalResult;
}
